"0","xyz_toks <- tokens("
"0","  xyz_corp, "
"0","  remove_punct = TRUE,              # On supprime à la volée la ponctuation"
"0","  remove_symbols = TRUE,            # On supprime à la volée les symboles"
"0","  remove_numbers = FALSE,           # On pourrait supprimer à la volée les nombres"
"0","  remove_separators = TRUE) |>      # On supprime les blancs laissés par la tokénisation"
"0","  tokens_split(separator = ""'"","
"0","               valuetype = ""fixed"") # On force la tokénisation à partir de l'apostrophe"
"0",""
"0","head(xyz_toks, 2)"
"1","Tokens consisting of "
"1",""
"1","2"
"1",""
"1"," document"
"1",""
"1","s"
"1"," and "
"1",""
"1","9"
"1",""
"1"," docvar"
"1",""
"1","s"
"1",".
"
"1","camille_deslauriers_1 :"
"1",""
"1","
"
"1"," [1]"
"1"," ""Elle""       "
"1"," ""l""          "
"1"," ""appellerait"""
"1"," ""Ozanne""     "
"1"," ""Ozanne""     "
"1"," ""Deux""       "
"1"," ""fois""       "
"1","
"
"1"," [8]"
"1"," ""née""        "
"1"," ""Hier""       "
"1"," ""morte""      "
"1"," ""au""         "
"1"," ""berceau""    "
"1","
"
"1","[ ... and "
"1",""
"1","925"
"1",""
"1"," more ]
"
"1","
"
"1","andre_berthiaume_2 :"
"1",""
"1","
"
"1"," [1]"
"1"," ""Depuis""  "
"1"," ""une""     "
"1"," ""douzaine"""
"1"," ""d""       "
"1"," ""années""  "
"1"," ""je""      "
"1"," ""consacre"""
"1"," ""une""     "
"1"," ""bonne""   "
"1","
"
"1","[10]"
"1"," ""partie""  "
"1"," ""de""      "
"1"," ""mon""     "
"1","
"
"1","[ ... and "
"1",""
"1","1,493"
"1",""
"1"," more ]
"
"1","
"
