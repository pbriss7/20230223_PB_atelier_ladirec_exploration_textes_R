---
title: "Exploration de données textuelles avec R"
author: "Pascal Brissette"
date: 2023-02-23
format:
  html:
    toc: true
    theme: yete
    fontsize: 1.1em
    linestretch: 1.7
toc: true
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

```

## Avant-propos

Vous trouverez dans le répertoire de travail, à l'intérieur du dossier `code/`, deux fichiers contenant le code de l'atelier, l'un portant l'extension **.qmd**, l'autre **.R**. Le premier, que vous avez ouvert et dont vous lisez actuellement le contenu, se présente comme une page HTML avec du texte suivi et des blocs de code. Si vous ouvrez l'autre fichier, vous verrez que le code est identique, mais que l'apparence est plus brute. En général, on utilise des documents Quarto, avec l'extension **.qmd**, lorsqu'on veut faire des présentations et qu'on souhaite ajouter au code à exécuter des paragraphes de commentaires. Un script **.R** de base est davantage fait pour exécuter du code. On peut certes y insérer des commentaires, mais on ne dispose d'aucun des outils d'édition auxquels on est habitués (tailles de titres, italiques, caractères gras, etc.). Il est tout de même bon de se familiariser avec ces deux formats. Le langage R peut également être exécuté dans des carnets Jupyter (*Jupyter Notebooks*), largement utilisés par la communauté scientifique.

### Exécuter le code dans un document Quarto

Lorsque vous cliquez sur le bouton **Render** dans le menu supérieur, un document est généré comprenant aussi bien le contenu que le résultat des blocs de code. Vous pouvez insérer du code comme ceci:

```{r}

# Création d'un vecteur comprenant trois titres de romans
romans <- c("Illusions perdues", "Le Dernier jour d'un condamné", "La Débâcle")

# Impression d'une chaine de caractères et d'une fonction qui choisit au hasard l'un des trois éléments du vecteur romans
cat("Mon roman préféré est ", sample(romans, 1), ".", sep = "")

```

Vous pouvez également ajouter des options. L'option `#| echo: false`, par exemple, permet d'imprimer le résultat d'une instruction dans le document généré avec **Render**, mais non le bloc de code. Cliquez sur **Render** et voyez le résultat.

```{r}
#| echo: false

romans <- c("Illusions perdues", "Le Dernier jour d'un condamné", "La Débâcle")

cat("Mon roman préféré est ", sample(romans, 1), ".", sep = "")

```

### Raccourci et aide

Pour exécuter tout un bloc de code, vous pouvez cliquer sur la flèche à l'extrémité supérieure droite du bloc. Si vous souhaitez plutôt exécuter les instructions d'un même bloc les unes après les autres, et observer le résultat, vous pouvez placer votre curseur au début de la ligne d'instruction à exécuter, puis appuyez sur 'COMMAND' + 'RETURN' (raccourci Mac) ou 'CTRL' + 'RETURN' (raccourci Windows) de votre clavier. Vous pouvez vous pratiquer avec les lignes d'instruction ci-dessous. Chacune appelle de l'aide sur une fonction en particulier. La fenêtre d'aide s'ouvrira dans la partie inférieure droite de RStudio.

```{r}
#| include: false
?paste()
?str()
?install.packages()
?nchar()
?data.frame()

```

### Nettoyage de l'environnement de travail (élimination des objets inutiles)

L'environnement de travail, auquel nous donne accès la fenêtre supérieure droite de RStudio, contient tous vos objets. Chaque fois que vous emmagasinez une donnée dans une variable, celle-ci est ajoutée à l'environnement. Cela prend peu de temps pour encombrer l'environnement (et la mémoire) de RStudio. Voici comment nettoyer partiellement ou entièrement cet environnement:

```{r}

# Création d'objets à éliminer
a=1
b=3
c="Virginie"
d="Théodore"

# Élimination d'une sélection d'éléments
rm(list = setdiff(ls(), c("d", "c")))

# Élimination de tout le contenu de l'environnement:
rm(list = ls()) 


```

## Préparation de l'environnement de travail

Il est d'usage de placer en haut de son script les extensions qu'on compte utiliser. Dans le bloc ci-dessous, vous trouvez une série d'instructions indiquant à R que si telle extension n'est pas installée déjà, il doit le faire (la structure de chaque ligne correspond à ce qu'on appelle "structure de contrôle"). Ensuite, chacune des extensions est activée individuellement. Il y a des manières plus rapides de faire cela, mais on déplie ici le processus par souci de clarté.

```{r}

# Installation des extensions dont nous aurons besoin (si elles ne sont pas déjà installées)
if(!"stringr" %in% rownames(installed.packages())) {install.packages("stringr")}
if(!"readxl" %in% rownames(installed.packages())) {install.packages("readxl")}
if(!"dplyr" %in% rownames(installed.packages())) {install.packages("dplyr")}
if(!"ggplot2" %in% rownames(installed.packages())) {install.packages("ggplot2")}
if(!"quanteda" %in% rownames(installed.packages())) {install.packages("quanteda")}
if(!"quanteda.textplots" %in% rownames(installed.packages())) {install.packages("quanteda.textplots")}
if(!"quanteda.textstats" %in% rownames(installed.packages())) {install.packages("quanteda.textstats")}
if(!"lsa" %in% rownames(installed.packages())) {install.packages("lsa")}

# Activation des extensions
library(readxl)               # Extension pour l'importation de fichiers Excel
library(stringr)              # Extension pour la manipulation des chaînes de caractères
library(dplyr)                # Extension pour la manipulation des structures des tableaux de données
library(ggplot2)              # Extension pour la production de graphiques de haute qualité
library(quanteda)             # Extension pour le forage textuel
library(quanteda.textplots)   # Extension pour le forage textuel
library(quanteda.textstats)   # Extension pour l'analyse des collocations
library(lsa)                  # Extension offrant un antidictionnaire élaboré

```

## Importation des données

Les données que nous utiliserons sont contenues dans un dossier séparé du répertoire. Il est judicieux de séparer les scripts, les données et les résultats dans trois dossiers différents. Les données proviennent d'un fichier Excel. Pour lire un tel fichier, il faut utiliser une extension spécialisée appelée **readxl**. Ci-dessous, on appelle la fonction `read_excel()` de cette extension et on lui donne, comme argument `path=`, le chemin conduisant vers le fichier. On lui indique également (argument `sheet=1`) que seule la première feuille de calcul doit être lue.

Le résultat de cette lecture est emmagasiné dans la variable `xyz`.

```{r}

# Lecture des données et assignation à une variable
# Les deux points suivis de / indiquent à R qu'il doit remonter d'un niveau pour trouver un dossier appelé `donnees`

xyz <- readxl::read_excel("../donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)


```

## Examen de l'objet

La différence entre un logiciel comme Excel et un environnement de développement comme RStudio, c'est qu'on ne voit pas d'emblée le contenu des objets qu'on importe ou qu'on crée. Il faut soit les appeler directement (on exécute le nom d'un objet), soit cliquer sur l'icône de la grille qui se trouve à droite du nom de l'objet dans la partie Environment (cela fera apparaître une fenêtre similaire à une feuille Excel), soit encore utiliser différentes fonctions qui renverront les informations utiles sur l'objet. Voici les principales:

```{r}
#| include: false

# Structure de l'objet
str(xyz)

# Nom des colonnes
colnames(xyz)

# Nombre de colonnes
ncol(xyz)

# Nom des lignes
rownames(xyz)

# Nombre de lignes
nrow(xyz)

# Nombre de lignes et de colonnes dans le tableau
dim(xyz)

# Type d'objet
class(xyz)

# Type de données d'un vecteur du tableau
class(xyz$`ISSN (numérique)`)
```

### Valeurs nulles et cellules sans contenu

On ne verra pas ici les différentes manières de traiter les valeurs nulles ou sans contenu, mais on peut au moins montrer comment les repérer. Dans R, une valeur nulle est indiquée par NA (*not available*). Cependant, vous quand vous travaillez avec des textes, vous pouvez avoir de nombreuses chaines de caractères tout simplement vides. On voit ci-dessous la manière de repérer ces deux types de valeurs.

```{r}
#| include: false

# La première fonction renvoie, pour chaque colonne du tableau, la somme des valeurs correspondant à NA.
sapply(xyz, function(x) sum(is.na(x)))

# Cette deuxième fonction renvoie, pour chacune des colonnes du tableau, la somme des valeurs correspondant à une chaîne de caractères vide.
sapply(xyz, function(x) sum(x == ""))


# Simple test: on peut créer deux colonnes sans contenu, puis vérifier à nouveau si R repère ces valeurs nulles
xyz$test <- NA
xyz$test2 <- ""


sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))


# On supprime ainsi des colonnes devenues inutiles
xyz[, c("test", "test2")] <- NULL


# Vérification que les colonnes ont bien disparu
colnames(xyz)


```

## Prétraitement des données

On appelle prétraitement les opérations qui doivent être faites aux données pour faciliter ou rendre possible leur manipulation. Le prétraitement peut impliquer une réduction des dimensions du jeu de données pour réduire le bruit ou simplement rendre moins lourd les processus ultérieurs. D'autres problèmes requièrent des opérations plus chronophages: corrections de chaînes de caractères, traitement des valeurs nulles, etc. Des logiciels gratuits tel OpenRefine sont de très puissants alliés à cette étape, même si tout peut être fait avec R.

### Principaux problèmes du jeu de données

-   Problème1: les noms de colonnes contiennent des accents et des espaces;

-   Problème 2: les observations ne sont pas pourvues d'identifiants uniques;

-   Problème 3: certaines colonnes semblent n'apporter aucune information essentielle;

-   Problème 4: on aimerait avoir une colonne réservée aux années;

-   Problème 5: on voudrait que les valeurs numériques soient considérées comme des nombres, non comme des chaines de caractères;

-   Problème 6: dans la colonne \`texte\`, les sauts de paragraphes du texte original sont indiqués par "\\n". Ce symbole ne sera pas utile ici et crée du bruit.

Traitons ces problèmes un par un.

Problème 1. Renommer les colonnes

```{r}

# On assigne aux noms de colonnes un vecteur comprenant, pour chaque colonne, le nom choisi, dans le bon ordre.
colnames(xyz) <- c("periodique", "titre", "auteur",
                   "numero", "date", "theme", "uri",
                   "editeur", "issn_imp", "issn_num",
                   "citation", "mention_legale", "texte")


```

Problème 2. Créer un identifiant unique. On peut utiliser les numéros de ligne.

```{r}

# On assigne à une nouvelle colonne appelée `doc_id` un vecteur numérique de 1 jusqu'au nombre de lignes que contient le tableau
xyz$doc_id <- 1 : nrow(xyz)


```

Problème 3. Repérer les colonnes inutiles et les éliminer

```{r}

# La première colonne semble contenir une information redondante. Vérifions:
unique(xyz[ , "periodique"])  # ou table(xyz$periodique)
unique(xyz[, "numero"])       # ou table(xyz$numero)
unique(xyz[, "theme"])        # ou table(xyz$theme)


# Exercice: poursuivez la vérification avec les colonnes `uri`, `editeur`, `issn` (x2), `mention_legale`. ###
# unique(...[, ...])

```

```{r}
# Solution au problème no 3

# On crée un vecteur avec les colonnes inutiles
colonnes_a_supprimer <- c("periodique", "editeur", "issn_imp", "issn_num", "mention_legale", "uri", "citation")


# On élimine l'ensemble des colonnes inutiles d'un seul coup.
xyz[, colonnes_a_supprimer] <- NULL

```

Problème 4. On veut créer une colonne contenant les années.

```{r}

# Il faut extraire les années de la colonne `date`. On utilise pour cela l'extension stringr.

xyz$annee <- stringr::str_extract(xyz$date, "[0-9]+")


### Exercice: éliminez la colonne `date`, qui ne sert plus à rien. ###
# xyz$... <- ...

```

Problème 5. Cette colonne \`annee\`, de même que la colonne \`numero\`, devraient être de type \`numeric\`, non de type \`chr\`.

```{r}

# La fonction as.integer() force la conversion du type chararcter en type integer.

xyz$numero <- as.integer(xyz$numero)      

# Faites la même opération pour la colonne appelée `numero`:

# xyz$... <- ...(xyz$...)


```

Problème 6. Remplacer un symbole dans une longue chaine de caractères (un texte).

```{r}

# Observons tout d'abord un texte en particulier
# xyz$texte[1]


# Remplaçons par une espace simple le symbole `\n`
xyz$texte <- gsub(pattern = "\n",
                  replacement = " ",
                  x = xyz$texte,
                  fixed = TRUE) # On indique ici à R de ne pas lire le "pattern" comme une                                       expression régulière, mais littéralement


### Exercice: vérifiez un texte pris au hasard pour voir s'il reste des scories
# xyz$...

```

Enfin, par souci de lisibilité, réordonnons la séquence des colonnes.

```{r}

xyz <- xyz[, c("doc_id", "auteur", "titre", "numero", "annee", "theme", "texte")]

```

## Exploration 1: les métadonnées

Maintenant que le tableau ne présente plus de problèmes apparents, on peut commencer à explorer les données. On veut pour l'essentiel se familiariser avec le contenu du tableau, observer des distributions, voir si des variables sont corrélées. Il est bon de séparer dans un premier temps l'exploration des métadonnées de celle des données textuelles.

Une première fonction très utile est `table()`. Celle-ci calcule le nombre de modalités ou valeurs d'une colonne. On peut ensuite faire des statistiques sur cette distribution.

```{r}

# La fonction `table( )` permet d'observer le nombre de modalités d'une variable. 
distrib_annuelle <- table(xyz$annee)

# Observons le résultat en appelant les 10 premiers éléments de L'objet `table`
distrib_annuelle[1:10]

# Quelle est la moyenne de cette distribution?
mean(distrib_annuelle)

### Exercice: créez une table pour observer la distribution des thèmes ###
# distrib_themes <- ...

```

Pour ordonner cette table en fonction des valeurs, on peut utiliser la fonction `sort( )`, auquel on passe l'argument `decreasing = TRUE` pour que les valeurs se présentent de manière décroissante.

```{r}

sort(distrib_annuelle, decreasing = TRUE)

# distrib_themes_ord <- sort(distrib_themes, decreasing = TRUE)


### Exercice: trouvez les noms des 10 principaux contributeurs de la revue ###
# distrib_auteurs <- ...
# distrib_auteurs_ord <- ...

```

Vous trouverez ci-dessous la réponse à la question posée ci-dessus (10 principaux contributeurs). Vous verrez que différentes syntaxes peuvent produire le même résultat.

```{r}
#| include: false

# 1. Syntaxe étendue (petite perte de mémoire, mais plus explicite: le résultat de chaque opération est emmagasiné dans une variable)
distrib_auteurs <- table(xyz$auteur)
distrib_auteurs_ord <- sort(distrib_auteurs, decreasing = TRUE)
head(distrib_auteurs_ord, n = 10)


# 2. Syntaxe condensée, par enchâssement des fonctions. Lire de l'intérieur vers l'extérieur
head(sort(table(xyz$auteur), decreasing = TRUE), 10)


# 3. Syntaxe enchainée (pipe), qui se lit de gauche à droite
xyz$auteur |> table() |> sort(decreasing = TRUE) |> head(10)


```

### Tests de corrélation

Au cours de l'exploration des données, on peut tenter de comprendre s'il existe des corrélations entre des variables. Les corrélations se calculent sur des variables numériques.

#### Test de corrélation avec des données numériques

Dans l'exemple ci-dessous, on utilise un jeu de données fourni par l'extension de base de R, `mtcars`. Celui-ci présente différentes marques et modèles de voitures et leurs attributs techniques, telle la distance parcourue par différents types de voitures selon le volume de leurs moteurs. On voudrait savoir s'il y a une corrélation, positive ou négative, entre le volume du moteur (disp) et la distance/gallon que peut parcourir une voiture.

```{r}

mtcars[1:3] # Dans le jeu de données, les noms de lignes (rownames) correspondent aux marques.

# Dans ce graphique simple, on pose la variable dépendante en y et l'indépendante en x
plot(mtcars$disp, mtcars$mpg)

```

Le test de corrélation est un test statistique qui mesure l'interdépendance ou l'association entre des paires de valeurs (deux variables). Plusieurs mesures permettent de vérifier la corrélation entre variables. Nous allons utiliser le coefficient appelé **tau de Kendall** (compris entre -1 et +1). Selon cette mesure, une corrélation positive est marquée par un tau positif, et inversement pour une corrélation négative. Plus le nombre s'éloigne de 0, plus la corrélation est forte. Vous pourrez lire l'article de Kendall [en ligne](https://academic.oup.com/biomet/article/30/1-2/81/176907). Avec les deux variables du jeu de données `mtcars`, le test montre que la variable dépendante est fortement corrélée à l'indépendante, et négative.

```{r}

cor.test(mtcars$disp, mtcars$mpg, method = "kendall")

```

#### Test de corrélation avec des données textuelles

Pour faire un test de corrélation avec des données textuelles, il faut créer des variables numériques qui seront mises en relation. Les chaines de caractères ne peuvent pas, telles quelles, être corrélées. Il faut choisir une mesure liée à la question de recherche. On prendra ici un exemple très simple et assez futile, simplement pour montrer le processus.

Nous allons vérifier s'il y a une corrélation, positive ou négative, entre la longueur des titres et la longueur des textes. Comme mesure, nous pouvons utiliser le nombre de mots ou de caractères que comportent les titres et les textes. Nous allons utiliser le nombrte de caractères des titres et textes par souci de simplicité. La somme de caractères pour chaque titre et chaque texte sera faite avec la fonction de base `nchar()`.

```{r}

xyz$ncharTitre <- nchar(xyz$titre)
xyz$ncharTexte <- nchar(xyz$texte)


# Observons le résultat dans la table
xyz[, c("ncharTitre", "ncharTexte")]


```

Procédons maintenant au test de corrélation

```{r}

cor.test(xyz$ncharTitre, xyz$ncharTexte, method = "kendall")


```

La mesure de corrélation, \`tau\`, est positive, mais très proche de zéro, ce qui dénote une corrélation très faible. On peut projeter ces données dans un diagramme à points

```{r}

ggplot(xyz, aes(x=ncharTitre, y=ncharTexte))+
  geom_jitter()+
  geom_smooth()
```

## Enrichissement des données

On peut tirer profit des données numériques ajoutées (longueur des titres et des textes) pour créer de nouveaux champs. Par exemple, on pourrait souhaiter classer les textes selon qu'ils sont "courts", "moyens" ou "longs". Un tel type de données est dit "catégorique". Le type d'objet que nous créerons pour emmagasiner ces données catégoriques s'appelle `factor`. Ci-dessous, nous allons choisir trois seuils aléatoires pour diviser nos données.

```{r}

# On peut tout d'abord observer le sommaire statistique et s'en inspirer
summary(xyz$ncharTexte)    

# Création de valeurs catégorielles fondées sur les modalités d'une autre colonne
# La fonction `factor()` possède un argument, `levels=` qui permet de déterminer l'ordre des catégories
xyz$longueur_texte_cat <- factor(                                
  ifelse(xyz$ncharTexte < 5000, "court",
         ifelse(xyz$ncharTexte >10000, "long", "moyen")),
  levels = c("court", "moyen", "long")                        
)

# Exercice: observez la distribution de cette variable avec la fonction table().

```

On peut observer ces proportions à l'aide d'un diagramme à barres.

```{r}

ggplot(xyz, aes(x = longueur_texte_cat))+                        
  geom_bar(stat = "count")

```

## Exploration 2: les données textuelles

Pour explorer les données textuelles à proprement parler, soit les textes de fiction contenus dans le tableau sous la variable `texte`, nous devons transformer l'objet de fond en comble. Comme on l'a vu ci-dessus avec les tests de corrélation, il est plus facile de demander à l'ordinateur de traiter des chiffres que des mots. Le texte sera donc transformé en une grande matrice d'occurrences de type documents-mots, où chaque ligne correspondra à un texte et chaque colonne, à l'un des mots du vocabulaire du corpus.

Une matrice est un objet qui ressemble beaucoup à un tableau de données, à ceci près que les valeurs dans les cellules sont toutes du même type. Dans notre grande matrice documents-mots, les valeurs seront numériques et correspondront au nombre de fois que tel mot en colonne apparaît dans tel texte en ligne. La représentation des textes qui en résulte est souvent appelée *Bag-Of-Words*, un sac de mots, car chaque texte est conçu comme un ensemble particulier de mots dont l'ordre d'apparition n'est pas pris en compte. Cette représentation repose sur l'idée qu'un texte qui contient un grand nombre de fois les mots "enfant", "parent" et "lien", par exemple, parlera vraisemblablement de la relation parent-enfant.

Pour faire face aux limites de ce modèle un peu grossier, mais aisé à concevoir, les spécialistes ont mis au point plusieurs techniques, telle l'application d'antidictionnaires aux matrices pour éliminer les mots fonctionnels ou sans intérêt dans un contexte de recherche spécifique. On peut également filtrer les matrices selon des seuils d'occurrences inférieurs ou supérieurs, on peut les pondérer pour augmenter le poids de mots (par exemple ceux qui sont particulièrement représentatifs d'un texte par rapport à tous les autres textes du corpus) et les normaliser (pour mieux comparer des textes de longueurs différentes). On peut également forger des n-grammes fondés sur des expressions d'intérêt pour la recherche ou sur des mesures de collocation. Nous ne ferons pas toutes ces opérations ici, mais il est important que vous sachiez que les limites inhérentes au *BOW* ont fait l'objet de nombreuses recherches et suscité des stratégies pour en neutraliser les effets.

Également, il est bon de savoir que d'autres représentations du texte ont fait leur apparition depuis le début du siècle (word2vec, BERT, etc.), dont on ne parlera pas ici. Le fameux modèle de langue GPT-3, derrière ChatGPT, est le dernier en date et repose sur un entrainement sur plusieurs milliards de mots.

Dans la suite de l'atelier, nous allons donc transformer le corpus textuel en une grande matrice d'occurrences de type documents-mots.

Pour faire l'ensemble des opérations, nous allons recourir à l'extension Quanteda, créée spécifiquement pour l'analyse statistique des textes. Il existe plusieurs autres extensions de ce type (udpipe, text2vec, tm, lsa, tidytext, etc.). On retient ici Quanteda en raison de la cohérence de la syntaxe et de la grande variété des analyses qu'elle permet.

On gagnera à consulter la page du *Comprehensive R Archive Network (CRAN)* consacrée au [*Natural Language Processing*](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html).

### Transformation de l'objet avec Quanteda

Le processus de transformation de l'objet tableau de données en grande matrice se fait, dans Quanteda, en trois grandes étapes.

1.  La première étape est la création d'un corpus avec la fonction corpus(). On y précise la colonne des identifiants uniques et la colonne contenant le texte à analyser;

2.  La deuxième étape est la tokénisation des textes du corpus. On peut à cette étape se servir d'un antidictionnaire pour éliminer les mots fonctionnels;

3.  La troisième étape est le passage entre cet objet "tokens" à la dfm (document-feature matrix).

Les métadonnées suivent par défaut chacune des transformations et permettront, une fois la dfm construite, de filtrer les documents en ligne avec les métadonnées.

Pour plus d'information sur chacune des fonctions, consultez la documentation. Exemple:

```{r}

?quanteda::corpus()

```

#### Premiière étape: création d'un objet corpus

Les arguments de la fonction `corpus()` permettent de préciser les colonnes du tableau correspondant aux identifiants uniques et aux textes.

```{r}

xyz_corp <- quanteda::corpus(xyz, docid_field = "doc_id", text_field = "texte")   

head(xyz_corp, 2)

```

#### Deuxième étape: création d'un objet tokens avec le corpus

La transformation du corpus en tokens correspond à la séparation des mots ou n-grammes de chaque texte. C'est ce qu'on appelle la tokénisation. Plusieurs opérations peuvent être faites à la volée.

```{r}
xyz_toks <- tokens(xyz_corp, 
                   remove_punct = TRUE,              # On supprime à la volée la ponctuation
                   remove_symbols = TRUE,            # On supprime à la volée les symboles
                   remove_numbers = FALSE,           # On pourrait supprimer à la volée les nombres
                   remove_separators = TRUE) |>      # On supprime les blancs laissés par la tokénisation
  tokens_split(separator = "'", valuetype = "fixed") # On force la tokénisation à partir de l'apostrophe

head(xyz_toks, 2)

```

#### Troisième étape: création d'un objet dfm à partir de l'objet tokens

```{r}

# On transforme l'objet tokens en dfm
xyz_dfm <- dfm(xyz_toks) |>
   # Retrait des mots fonctionnels avec l'antidictionnaire lsa (inspecter le dictionnaire!)
  dfm_remove(lsa::stopwords_fr) |>  
  # Un mot doit être présent dans au moins 15% des documents (élimination des hapax)
  dfm_trim(min_docfreq = 0.15,
  # Un mot ne doit pas être présent dans plus de 70% des documents du corpus
           max_docfreq = 0.7, 
  # L'argument suivant précise que la valeur indiquée dans min_docfreq= est une proportion
           docfreq_type = "prop")          

head(xyz_dfm, 2)

### Exercice: modifiez les seuils et voyez l'effet sur les dimensions de l'objet!
# Pour voir les dimensions de l'objet, vous n'avez qu'à l'appeler ainsi:



```

### Exploration du vocabulaire de sous-ensembles de textes

L'extension Quanteda rend aisée l'exploration du vocabulaire de sous-ensembles de textes. Ces sous-ensembles sont générés grâce aux métadonnées que les objets corpus-tokens-dfm portent avec eux. La fonction `dfm_subset()` a un argument `subset=` qui permet d'insérer une expression formelle agissant comme filtre.

Dans le premier exemple, nous allons explorer les vocabulaires (filtrés) des textes parus en 2015 et en 2021. Le résultat sera transposé en un nuage de mots où ceux qui sont les plus fréquents seront magnifiés et positionnés au centre du nuage.

```{r}

          
xyz_dfm_rioux <- dfm_subset(xyz_dfm, subset = auteur == "Hélène Rioux")
xyz_dfm_dorais <- dfm_subset(xyz_dfm, subset = auteur == "David Dorais")

set.seed(100)
textplot_wordcloud(xyz_dfm_rioux, max_words = 200)
set.seed(100)
textplot_wordcloud(xyz_dfm_dorais, max_words = 200)



```

Si on veut mieux comparer les deux sous-groupes, on peut utiliser le code suivant, un peu plus sophistiqué.

```{r}

dfm_comp_auteurs <- xyz_corp |> 
  
  corpus_subset(auteur %in% c("Hélène Rioux", "David Dorais")) |> 
  
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = FALSE,
         remove_separators = TRUE) |>     
  tokens_split(separator = "'", valuetype = "fixed") |> 
  
  dfm()|>                
  
  dfm_remove(lsa::stopwords_fr) |>        
  
  dfm_trim(min_docfreq = 0.15,            
           max_docfreq = 0.7,           
           docfreq_type = "prop")


dfm_comp_auteurs_groupes <- dfm_group(dfm_comp_auteurs, dfm_comp_auteurs$auteur)

textplot_wordcloud(dfm_comp_auteurs_groupes, comparison = TRUE, max_words = 300,
                   color = c("blue", "red"))


```

On pourrait maintenant souhaiter comparer les vocabulaires dominants de numéros thématiques.

```{r}

xyz_dfm_th_jardin <- dfm_subset(xyz_dfm, subset = theme == "Jardin : un enfer de morceaux de paradis")

xyz_dfm_th_YOLO <- dfm_subset(xyz_dfm, subset = theme == "YOLO (You Only Live Once) : hardis, téméraires, écervelés, aventureux, fonceurs, délurés")


# Comparaison des vocabulaires de numéros thématiques
set.seed(100)
textplot_wordcloud(xyz_dfm_th_jardin)
set.seed(100)
textplot_wordcloud(xyz_dfm_th_YOLO)



#### Exercice: choisissez deux autres numéros thématiques et comparez les vocabulaires. Vous pouvez utiliser la voie simple ou reprendre le bloc d'instructions enchainées ci-dessus.
```

### Réseaux des cooccurrences

L'exploration de textes fondée sur la fréquence (brute, pondérée ou normalisée) et la technique du "sac de mots" (*BOW*) ne dit rien du contexte immédiat dans lequel les mots sont plongés. Dans le nuage de mots généré à partir des textes de l'auteur David Dorais, on a vu que "jeune" et "femme" sont les mots dominants, suivis de près par "homme". De là à dire que les textes de Dorais s'intéressent à la "jeune femme", il y a un pas qu'on ne peut franchir avant d'avoir vérifié si les mots apparaissent bel et bien, de manière récurrente, dans le même contexte. L'une des manières de se rapprocher du texte et de vérifier quels couples de mots s'attirent, apparaissent souvent dans un même contexte, est de créer un **réseau de cooccurrence**.

Selon Kboubi, Habacha, and BenAhmed (2010), "un réseau de cooccurrence est un réseau de termes où chaque noeud représente un terme et un arc entre deux nœuds représente la relation de cooccurrence entre les deux termes concernés. Ce réseau permet d'identifier les termes qui apparaissent souvent ensemble au sein d'une même fenêtre mais pas nécessairement juxtaposés."

Le réseau de cooccurrence ne permet pas encore de savoir si "jeune" et "femme" se suivent dans cet ordre dans les textes de D. Dorais, mais il permettra, à mi-chemin entre le "sac de mots" et la plongée dans le texte par une lecture rapprochée, de vérifier les associations.

L'extension Quanteda offre une fonction qui permet, à partir d'un objet tokens ou dfm, de construire une matrice de cooccurrence qui puisse être projetée sous forme de diagramme de réseau. C'est cette fonction, fcm(), que nous utiliserons ci-dessous. Nous allons devoir réduire considérablement les dimensions de cette matrice en utilisant un nombre réduit de cooccurrences -- les plus fréquentes --, sans quoi le diagramme sera impossible à déchiffrer. Mais vous pouvez modifier tous les paramètres et observer les effets sur le diagramme.

```{r}

  # On fournit en entrée l'objet tokens créé auparavant
xyz_fcm <- xyz_toks |>
  # On ne retient que les tokens trouvés dans les textes d'un auteur
  tokens_subset(subset = auteur == "David Dorais") |> 
  # Retrait des mots fonctionnels avec l'antidictionnaire lsa
  tokens_remove(lsa::stopwords_fr) |>
  # On choisit ici une fenêtre contextuelle de 10 mots
  fcm(context = "window", window = 10, tri = FALSE)


# On repère ici couples de mots ayant un coefficient de cooccurrence égal ou supérieur à 30
principaux_mots <- names(topfeatures(xyz_fcm, 30))


# La matrice de cooccurrence est réduite à ses principaux mots, et projetée sous la forme d'un diagramme de réseau
set.seed(100)
textplot_network(fcm_select(xyz_fcm, principaux_mots), min_freq = 0.7)
# On augmente encore le seuil pour rendre le graphique plus facile à interpréter



```

L'épaisseur du lien et la proximité entre les mots "jeune" et "femme" montrent que les mots s'appellent fortement. On peut retourner à la matrice pour voir exactement combien de fois les deux termes cooccurrent dans une fenêtre de 10 mots.

```{r}

# Conversion de l'objet fcm en simple matrice
xyz_cooc_matrice <- convert(xyz_fcm, "matrix")

# On peut observer la structure de cet objet. On voit que les noms de lignes et de colonnes correspondent aux mots des textes
str(xyz_cooc_matrice)

# On peut donc indexer la matrice avec les mots à observer. La valeur renvoyée correspondra au nombre de fois que ces mots cooccurrent dans la fenêtre de 10 mots.
xyz_cooc_matrice["jeune", "femme"]
```

### Au plus près des textes: le concordancier

On peut souhaiter se rapprocher davantage encore de la texture des mots et vérifier *de visu*, sans pour autant lire intégralement tous les textes, si "jeune" et "femme" se suivent dans cet ordre et, le cas échéant, quels mots viennent tout juste avant et après la cooccurrence. On utilisera alors un outil bien connu, le concordancier, que Quanteda met à notre portée avec la fonction `kwic()`. Cette fonction, dont le nom correspond à *keywords-in-context*, peut prendre en entrée un objet **corpus** ou encore un objet **tokens**. Un argument, `window=`, permet d'indiquer le nombre de mots que l'on souhaite obtenir de chaque côté du mot pivot. Ce dernier est précisé à l'aide de l'argument `pattern=`, ce qui veut dire que vous pouvez utiliser une expression régulière (*regex*) pour attraper différentes formes d'un mot.

```{r}

xyz_corp |> 

  corpus_subset(subset = auteur == "David Dorais") |> 
  
  tokens(remove_punct = TRUE,
         remove_symbols = TRUE, 
         remove_numbers = FALSE, 
         remove_separators = TRUE) |>
  
  # tokens_remove(lsa::stopwords_fr) |>
  
  kwic(pattern = phrase("jeunes? femmes?"), 
       window = 3, 
       valuetype = "regex") |> head()


```

Ce concordancier est la dernière étape de notre exploration, mais plusieurs autres techniques sont à votre portée pour explorer et analyser vos données textuelles!

Pour toute question ou commentaire: pascal.brissette\@mcgill.ca

## Bibliographie

Baillargeon, Sophie. s. d. « R pour scientifique ». Consulté le 31 août 2022. <https://stt4230.rbind.io/>.

Baruffa, Oscar. s. d. *Big Book of R*. Consulté le 1 septembre 2022. <https://www.bigbookofr.com/>.

Claveau, Vincent, Ewa Kijak, et Olivier Ferret. s. d. « Explorer le graphe de voisinage pour améliorer les thésaurus distributionnels ».

Digital Scholarship Hub - McGill University, réal. 2022. *Planning Your Text Analysis Project*. <https://www.youtube.com/watch?v=6DNmoRHRQ-g>.

Goulet, Vincent. s. d. « Introduction à R ». Consulté le 29 août 2022. <https://vigou3.github.io/raquebec-atelier-introduction-r/>.

Grimmer, Justin. 2022. *Text as Data: A New Framework for Machine Learning and the Social Sciences*. Princeton.

Guay, Jean-Herman. s. d. « Statistiques en sciences humaines et sociales avec R ». Consulté le 19 août 2022. <https://dimension.usherbrooke.ca/>.

Jockers, Matthew L. 2014. *Text Analysis with R for Students of Literature*. Quantitative Methods in the Humanities and Social Sciences. Springer International Publishing. <https://doi.org/10.1007/978-3-319-03164-4>.

Kboubi, Férihane, Anja Habacha Chabi, et Mohamed BenAhmed. s. d. « L'exploitation des relations d'association de termes pour l'enrichissement de l'indexation de documents textuels ».

Lebart, Ludovic, Bénédicte Pincemin, et Céline Poudat. 2019. *Analyse des données textuelles*. Mesure et évaluation 11. Québec: Presses de l'Université du Québec.

Paradis, Emmanuel. 2002. « R pour les débutants ». <https://cran.r-project.org/doc/contrib/Paradis-rdebuts_fr.pdf>.

Series, QCBS R. Workshop. s. d. « Ateliers du Centre québécois des sciences de la biodiversité ». Consulté le 20 août 2022. <https://r.qcbs.ca/fr/>.

Silge, Julia, et David Robinson. s. d. *Text Mining with R*. Consulté le 6 mars 2020. <https://www.tidytextmining.com/>.
