---
title: "Exploration de données textuelles avec R"
author: "Pascal Brissette"
date: 2023-02-23
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

```

## Avant-propos

Les lignes d'instruction (ou lignes de code) contenues dans le présent document, **xyz.qmd**, et dans le script **xyz.R** du même dossier sont à peu identiques. La différence entre les deux fichiers, c'est que l'un permet de tirer profit du langage HTML et de présenter un texte suivi avec des titres, des sous-titres, des italiques, des caractères gras, des listes, etc. Dans un script .R, au contraire, les possibilités sont limitées, le script étant d'abord fait pour rassembler des lignes d'instructions qui seront exécutées. Les commentaires généralement courts qu'on trouve dans un script sont précédés d'un ou de plusieurs croisillons #. Cela est plus rebutant à la lecture, mais il est bon de se familiariser avec ces deux formats. Un script .R est plus léger et c'est généralement le format qu'on utilise pour travailler. On recourt au document Quarto (.qmd) pour des présentations devant public. Un aspect très intéressant d'un tel document .qmd est qu'on peut générer, à partir de son contenu, un document .pdf, .word ou .html.

### Exécuter le code d'un document Quarto

Lorsque vous cliquez sur le bouton **Render** dans le menu supérieur, un document sera généré comprenant aussi bien le contenu que le résultat des blocs de code. Vous pouvez insérer du code comme ceci:

```{r}

romans <- c("Illusions perdues", "Le Dernier jour d'un condamné", "La Débâcle")

cat("Mon roman préféré est ", sample(romans, 1), ".", sep = "")

```

Vous pouvez également ajouter des options. L'option `#| echo: false`, par exemple, permet d'imprimer le résultat d'une instruction dans le document généré avec **Render**, mais non le bloc de code. Cliquez sur **Render** et voyez le résultat.

```{r}
#| echo: false

romans <- c("Illusions perdues", "Le Dernier jour d'un condamné", "La Débâcle")

cat("Mon roman préféré est ", sample(romans, 1), ".", sep = "")

```

### Raccourci et aide

Pour exécuter tout un bloc de code, vous pouvez cliquer sur la flèche à l'extrémité supérieure droite du bloc. Si vous souhaitez plutôt exécuter les instructions d'un même bloc les unes après les autres, et observer le résultat, vous pouvez placer votre curseur au début de la ligne d'instruction à exécuter et appuyez sur 'COMMAND' + 'RETURN' (raccourci Mac) ou 'CTRL' + 'RETURN' (raccourci Windows) de votre clavier. Vous pouvez vous pratiquer avec les lignes d'instruction ci-dessous. Chacune appelle de l'aide sur une fonction en particulier. La fenêtre d'aide s'ouvrira dans la partie inférieure droite de RStudio.

```{r}

?paste()
?str()
?install.packages()
?library()
?nchar()
?data.frame()

```

### Nettoyage de l'environnement de travail (élimination des objets inutiles)

L'environnement de travail, auquel nous donne accès l'onglent Environment de RStudio (coin supérieur droit), contient tous vos objets. Chaque fois que vous emmagasinez une donnée dans une variable, celle-ci est ajoutée à l'environnement. Cela prend peu de temps pour charger l'environnement (et la mémoire) de RStudio. Voici comment nettoyer partiellement ou entièrement cet environnement:

```{r}

# Création d'objets à éliminer
a=1
b=3
c="Virginie"
d="Théodore"

# Élimination de tout le contenu de l'environnement:

rm(list = ls()) 


# Élimination d'une sélection d'éléments
rm(list = setdiff(ls(), c("d")))


```

## Préparation de l'environnement de travail

Il est d'usage de placer en haut de son script les extensions qu'on compte utiliser. On voit d'abord une série d'instructions indiquant à R que si telle extension n'est pas installée déjà, il doit le faire. Ensuite, chacune des extensions est activée individuellement. Il y a des manières plus rapides de faire cela, mais on déplie ici le processus pour être clair.

```{r}

# Installation des extensions dont nous aurons besoin (si elles ne sont pas déjà installées)
if(!"stringr" %in% rownames(installed.packages())) {install.packages("stringr")}
if(!"readxl" %in% rownames(installed.packages())) {install.packages("readxl")}
if(!"dplyr" %in% rownames(installed.packages())) {install.packages("dplyr")}
if(!"ggplot2" %in% rownames(installed.packages())) {install.packages("ggplot2")}
if(!"quanteda" %in% rownames(installed.packages())) {install.packages("quanteda")}
if(!"quanteda.textplots" %in% rownames(installed.packages())) {install.packages("quanteda.textplots")}
if(!"quanteda.textstats" %in% rownames(installed.packages())) {install.packages("quanteda.textstats")}
if(!"lsa" %in% rownames(installed.packages())) {install.packages("lsa")}

# Activation des extensions
library(readxl)               # Extension pour l'importation de fichiers Excel
library(stringr)              # Extension pour la manipulation des chaînes de caractères
library(dplyr)                # Extension pour la manipulation des structures des tableaux de données
library(ggplot2)              # Extension pour la production de graphiques de haute qualité
library(quanteda)             # Extension pour le forage textuel
library(quanteda.textplots)   # Extension pour le forage textuel
library(quanteda.textstats)   # Extension pour l'analyse des collocations
library(lsa)                  # Extension offrant un antidictionnaire élaboré

```

## Importation des données

Les données que nous utiliserons sont contenues dans un dossier séparé du répertoire. Il est judicieux de séparer les scripts, les données et les résultats dans trois dossiers différents. Les données sont emmagasinées dans un fichier Excel. Pour lire un tel fichier, il faut utiliser une extension spécialisée appelée **readxl**. Ci-dessous, on appelle la fonction read_excel() de cette extension et on lui donne, comme argument **path=**, le chemin conduisant vers le fichier. On lui indique également (argument **sheet=1**) que la première feuille de calcul seule est à lire.

Le résultat de cette lecture est emmagasiné dans la variable xyz.

```{r}

# Lecture des données et assignation à une variable 
xyz <- readxl::read_excel("../donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)


```

## Examen de l'objet

La différence entre un logiciel comme Excel et un environnement de développement comme RStudio, c'est qu'on ne voit pas d'emblée le contenu des objets qu'on importe ou qu'on crée. Il faut soit les appeler directement (on exécute le nom d'un objet), soit cliquer sur l'icône de la grille qui se trouve à droite du nom de l'objet dans la partie Environment, soit encore utiliser différentes fonctions qui renverront les informations utiles sur l'objet. Voici les principales:

```{r}
#| include: false

# Structure de l'objet
str(xyz)

# Nom des colonnes
colnames(xyz)

# Nombre de colonnes
ncol(xyz)

# Nom des lignes
rownames(xyz)

# Nombre de lignes
nrow(xyz)

# Nombre de lignes et de colonnes dans le tableau
dim(xyz)

# Type d'objet
class(xyz)

# Type de données d'un vecteur du tableau
class(xyz$`ISSN (numérique)`)
```

### Valeurs nulles et cellules sans contenu

On ne verra pas ici les différentes manières de traiter les valeurs nulles ou sans contenu, mais on peut au moins montrer comment les repérer. Dans R, une valeur nulle est indiquée par NA (*not available*). Cependant, vous quand vous travaillez avec des textes, vous pouvez avoir de nombreuses chaines de caractères tout simplement vides. On voit ci-dessous la manière de repérer ces deux types de valeurs.

```{r}
# La première fonction renvoie, pour chaque colonne du tableau, la somme des valeurs correspondant à NA.
sapply(xyz, function(x) sum(is.na(x)))

# Cette deuxième fonction renvoie, pour chacune des colonnes du tableau, la somme des valeurs correspondant à une chaîne de caractères vide.
sapply(xyz, function(x) sum(x == ""))


# Simple test: on peut créer deux colonnes sans contenu, puis vérifier à nouveau si R repère ces valeurs nulles
xyz$test <- NA
xyz$test2 <- ""


sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))


# On supprime ainsi des colonnes devenues inutiles
xyz[, c("test", "test2")] <- NULL


# Vérification que les colonnes ont bien disparu
colnames(xyz)


```

## Prétraitement des données

On appelle prétraitement les opérations qui doivent être faites aux données métadonnées pour faciliter ou rendre possible leur manipulation. Le prétraitement peut impliquer une réduction des dimensions du jeu de données pour réduire le bruit ou simplement rendre moins lourd les processus ultérieurs. D'autres problèmes requièrent des opérations plus chronophages: corrections de chaînes de caractères, traitement des valeurs nulles, etc. Des logiciels gratuits tel OpenRefine sont de très puissants alliés à cette étape, même si tout peut être fait avec R.

### Principaux problèmes du jeu de données

-   Problème1: les noms de colonnes contiennent des accents et des espaces;

-   Problème 2: les observations ne sont pas pourvues d'identifiants uniques;

-   Problème 3: certaines colonnes semblent n'apporter aucune information essentielle;

-   Problème 4: on aimerait avoir une colonne réservée aux années;

-   Problème 5: on voudrait que les valeurs numériques soient considérées comme des nombres, non comme des chaines de caractères;

-   Problème 6: dans la colonne \`texte\`, les sauts de paragraphes du texte original sont indiqués par "\\n".

Traitons ces problèmes un par un.

Problème 1. Renommer les colonnes

```{r}


colnames(xyz) <- c("periodique", "titre", "auteur",
                   "numero", "date", "theme", "uri",
                   "editeur", "issn_imp", "issn_num",
                   "citation", "mention_legale", "texte")


```

Problème 2. Créer un identifiant unique. On peut utiliser les numéros de ligne.

```{r}


xyz$doc_id <- 1:nrow(xyz)


```

Problème 3. Repérer les colonnes inutiles et les éliminer

```{r}

# La première colonne semble contenir une information redondante. Vérifions:
unique(xyz[ , "periodique"])  # ou table(xyz$periodique)
unique(xyz[, "numero"])       # ou table(xyz$numero)
unique(xyz[, "theme"])        # ou table(xyz$theme)


# Exercice: poursuivez la vérification avec les colonnes `uri`, `editeur`, `issn` (x2), `mention_legale`. ###
# unique(...[, ...])

```

```{r}
# Solution au problème no 3

# Créons un vecteur avec les colonnes inutiles (on utilise pour cela la fonction de concaténation c( ) )
colonnes_a_supprimer <- c("periodique", "editeur", "issn_imp", "issn_num", "mention_legale", "uri", "citation")


# On élimine l'ensemble des colonnes inutiles d'un seul coup.
xyz[, colonnes_a_supprimer] <- NULL

```

Problème 4. On veut créer une colonne contenant les années.

```{r}

# Il faut extraire les années de la colonne `date`. On utilise pour cela l'extension stringr.

xyz$annee <- stringr::str_extract(xyz$date, "[0-9]+")


### Exercice: éliminez la colonne `date`, qui ne sert plus à rien. ###
# xyz$... <- ...

```

Problème 5. Cette colonne \`annee\`, de même que la colonne \`numero\`, devraient être de type \`numeric\`, non de type \`chr\`.

```{r}

# La fonction as.integer() force la conversion du type chararcter en type integer.

xyz$numero <- as.integer(xyz$numero)      

# Faites la même opération pour la colonne appelée `numero`:

# xyz$... <- ...(xyz$...)


```

Problème 6. Remplacer un symbole dans une longue chaine de caractères (un texte).

```{r}

# Observons tout d'abord un texte en particulier
# xyz$texte[1]


# Remplaçons par une espace simple le symbole `\n`
xyz$texte <- gsub(pattern = "\n", replacement = " ", x = xyz$texte, fixed = TRUE)


### Exercice: vérifiez un texte pris au hasard pour voir s'il reste des scories
# xyz$...

```

Enfin, par souci de lisibilité, réordonnons la séquence des colonnes.

```{r}

xyz <- xyz[, c("doc_id", "auteur", "titre", "numero", "annee", "theme", "texte")]

```

## Exploration 1: les métadonnées

Maintenant que le tableau ne présente plus de problèmes apparents, on peut commencer à explorer les données. On veut pour l'essentiel se familiariser avec le contenu du tableau, observer des distributions, voir si des variables sont corrélées. Il est bon de séparer dans un premier temps l'exploration des métadonnées de celle des données textuelles.

Une première fonction très utile est `table()`. Celle-ci calcule le nombre de modalités ou valeurs d'un vecteur-colonne. On peut ensuite faire des statistiques sur cette distribution.

```{r}
# La fonction `table( )` permet d'observer le nombre de modalités d'une variable. 
distrib_annuelle <- table(xyz$annee)
distrib_annuelle

# Quelle est la moyenne de cette distribution?
mean(distrib_annuelle)

### Exercice: créez une table pour observer la distribution des thèmes ###
# distrib_themes <- ...


```

Pour ordonner cette table en fonction des valeurs, on peut utiliser la fonction `sort( )`, auquel on passe l'argument `decreasing = TRUE` pour que les valeurs se présentent de manière décroissante.

```{r}

distrib_themes_ord <- sort(distrib_themes, decreasing = TRUE)


### Exercice: trouvez les noms des 10 principaux contributeurs de la revue ###
# distrib_auteurs <- ...
# distrib_auteurs_ord <- ...

```

Vous trouverez ci-dessous la réponse à la question posée ci-dessus. Vous verrez que différentes syntaxes peuvent produire le même résultat.

```{r}

# 1. Syntaxe étendue (petite perte de mémoire, mais plus explicite: le résultat de chaque opération est emmagasiné dans une variable)
distrib_auteurs <- table(xyz$auteur)
distrib_auteurs_ord <- sort(distrib_auteurs, decreasing = TRUE)
head(distrib_auteurs_ord, n = 10)


# 2. Syntaxe condensée, par enchâssement des fonctions. Lire de l'intérieur vers l'extérieur
head(sort(table(xyz$auteur), decreasing = TRUE), 10)


# 3. Syntaxe enchainée (pipe), qui se lit de gauche à droite
xyz$auteur |> table() |> sort(decreasing = TRUE) |> head(10)


```

### Tests de corrélation

Au cours de l'exploration des données, on peut tenter de comprendre s'il existe des corrélations entre des variables. Les corrélations se calculent sur des variables numériques.

#### Test de corrélation avec des données numériques

Dans l'exemple ci-dessous, on utilise un jeu de données fourni par l'extension de base de R, `mtcars`. Celui-ci présente différentes marques et modèles de voitures et leurs attributs techniques, telle la distance parcourue par différentes marques selon le volume de leurs moteurs. On voudrait savoir s'il y a une corrélation, positive ou négative, entre le volume du moteur (disp) et la distance/gallon que peut parcourir une voiture.

```{r}

mtcars # Dans le jeu de données, les noms de lignes (rownames) correspondent aux marques.

# Dans ce graphique simple, on pose la variable dépendante en y et l'indépendante en x
plot(mtcars$disp, mtcars$mpg)

```

Le test de corrélation est un test statistique qui mesure l'interdépendance ou l'association entre des paires de valeurs (deux variables). Plusieurs mesures permettent de vérifier la corrélation entre variables. Nous allons utiliser le coefficient appelé tau de Kendall (compris entre -1 et +1). Selon cette mesure, une corrélation positive est marquée par un tau positif, et inversement pour une corrélation négative. Plus le nombre s'éloigne de 0, plus la corrélation est forte. Vous pourrez lire l'article de Kendall [en ligne](https://academic.oup.com/biomet/article/30/1-2/81/176907). Avec les deux variables du jeu de données `mtcars`, le test montre que la variable dépendante est fortement corrélée à l'indépendante, et négative.

```{r}

cor.test(mtcars$disp, mtcars$mpg, method = "kendall")
```

#### Test de corrélation avec des données textuelles

Pour faire un test de corrélation avec des données textuelles, il faut créer des variables numériques qui seront mises en relation. Les chaines de caractères ne peuvent pas, telles quelles, être corrélées. Il faut choisir une mesure liée à la question de recherche. On prendra ici un exemple très simple et assez futile, simplement pour indiquer le processus.

Nous allons vérifier s'il y a une corrélation, positive ou négative, entre la longueur des titres et la longueur des textes. Comme mesure, nous pouvons utiliser le nombre de mots que comportent les titres et les textes, ou encore le nombre de caractères de chaque variable. Nous allons retenir cette dernière option par souci de simplicité. Les sommes de caractères pour chaque titre et chaque texte sera faite avec la fonction de base `nchar()`.

```{r}

xyz$ncharTitre <- nchar(xyz$titre)
xyz$ncharTexte <- nchar(xyz$texte)


# Observons le résultat dans la table
xyz[, c("ncharTitre", "ncharTexte")]


```

Procédons maintenant au test de corrélation

```{r}

cor.test(xyz$ncharTitre, xyz$ncharTexte, method = "kendall")


```

La mesure de corrélation, \`tau\`, est positive, mais très proche de zéro, ce qui dénote une corrélation très faible. On peut projeter ces données dans un diagramme à points

```{r}

ggplot(xyz, aes(x=ncharTitre, y=ncharTexte))+
  geom_jitter()+
  geom_smooth()
```

## Enrichissement des données

On peut tirer profit des données numériques ajoutées (longueur des titres et des textes) pour créer de nouveaux champs. Par exemple, on pourrait souhaiter classer les textes selon qu'ils sont "courts", "moyens" ou "longs". Un tel type de données est dit "catégorique". Le type d'objet que nous créerons pour emmagasiner ces données catégoriques s'appelle `factor`. Ci-dessous, nous allons choisir trois seuils aléatoires pour diviser nos données.

```{r}

# On peut tout d'abord observer le sommaire statistique et s'en inspirer
summary(xyz$ncharTexte)    

xyz$longueur_texte_cat <- factor(                                # Création de valeurs catégorielles fondées sur les modalités d'une autre colonne
  ifelse(xyz$ncharTexte < 5000, "court",
         ifelse(xyz$ncharTexte >10000, "long", "moyen")),
  levels = c("court", "moyen", "long")                           # La fonction `factor()` possède un argument, `levels=` qui permet de déterminer l'ordre des catégories
)

# Exercice: observez la distribution de cette variable avec la fonction table().

```

On peut observer ces proportions à l'aide d'un diagramme à barres.

```{r}

ggplot(xyz, aes(x = longueur_texte_cat))+                        
  geom_bar(stat = "count")

```

## Exploration 2: les données textuelles

Pour explorer les données textuelles à proprement parler, soit les textes de fiction contenus dans le tableau sous la variable `texte`, nous devons transformer l'objet de fond en comble. Comme on l'a vu ci-dessus avec les tests de corrélation, il est plus facile de demander à l'ordinateur de traiter des chiffres que des mots. Le texte sera donc transformé en une grande matrice d'occurrences de type documents-mots, où chaque ligne correspondra à un texte et chaque colonne, à l'un des mots du vocabulaire du corpus. Une matrice est un objet qui ressemble beaucoup à un tableau de données, à ceci près que les valeurs dans les cellules sont toutes identiques. Dans notre grande matrice documents-mots, les valeurs correspondront au nombre de fois que tel mot en colonne apparaît dans tel texte en ligne. La représentation des textes qui en résulte est souvent appelée *Bag-Of-Words*, un sac de mots, car chaque texte est conçu comme un ensemble particulier de mots dont l'ordre d'apparition n'est pas pris en compte. Cette représentation repose sur l'idée qu'un texte qui contient un grand nombre de fois les mots "enfant", "parent" et "lien" parlera vraisemblablement de la relation parent-enfant.

Pour faire face aux limites de ce modèle un peu grossier, mais aisé à concevoir, les spécialistes ont mis au point plusieurs techniques, telle l'application d'antidictionnaires aux matrices pour éliminer les mots fonctionnels ou sans intérêt dans un contexte de recherche spécifique. On peut également filtrer les matrices selon des seuils d'occurrences inférieurs ou supérieurs, on peut les pondérer pour augmenter le poids de mots (par exemple ceux qui sont particulièrement représentatifs d'un texte par rapport à tous les autres textes du corpus) et les normaliser (pour mieux comparer des textes de longueurs différentes). On peut également forger des n-grammes fondés sur des expressions d'intérêt pour la recherche ou sur des mesures de collocation. Nous ne ferons pas toutes ces opérations ici, mais il est important que vous sachiez que les limites inhérentes au *BOW* ont fait l'objet de nombreuses recherches et suscité des stratégies pour en neutraliser les effets.

Également, il est bon de savoir que d'autres représentations du texte ont fait leur apparition depuis le début du siècle (word2vec, BERT, etc.), dont on ne parlera pas ici. Le fameux modèle de langue GPT-3, derrière ChatGPT, est le dernier en date et repose sur un entrainement sur plusieurs milliards de mots.

Dans la suite de l'atelier, nous allons donc transformer le corpus textuel en une grande matrice d'occurrences de type documents-mots.

Pour faire l'ensemble des opérations, nous allons recourir à l'extension Quanteda, créée spécifiquement pour l'analyse statistique des textes. Il existe plusieurs autres extensions de ce type (udpipe, text2vec, tm, lsa, tidytext, etc.). On retient ici Quanteda en raison de la cohérence de la syntaxe et de la grande variété des analyses qu'elle permet.

On gagnera à consulter la page du *Comprehensive R Archive Network (CRAN)* consacrée au [*Natural Language Processing*](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html).

### Transformation de l'objet avec Quanteda

Le processus de transformation de l'objet tableau de données en grande matrice se fait, dans Quanteda, en trois grandes étapes.

1.  La première étape est la création d'un corpus avec la fonction corpus(). On y précise la colonne des identifiants uniques et la colonne contenant le texte à analyser;

2.  La deuxième étape est la tokénisation des textes du corpus. On peut à cette étape se servir d'un antidictionnaire pour éliminer les mots fonctionnels;

3.  La troisième étape est le passage entre cet objet "tokens" à la dfm (document-feature matrix).

Les métadonnées suivent par défaut chacune des transformations et permettront, une fois la dfm construite, de filtrer les documents en ligne avec les métadonnées.

Pour plus d'information sur chacune des fonctions, consultez la documentation. Exemple:

```{r}
?quanteda::corpus()
```

#### Premiière étape: création d'un objet corpus

Les arguments de la fonction `corpus()` permettent de préciser les colonnes du tableau correspondant aux identifiants uniques et aux textes.

```{r}

xyz_corp <- quanteda::corpus(xyz, docid_field = "doc_id", text_field = "texte")   

head(xyz_corp, 2)
```

#### Deuxième étape: création d'un objet tokens avec le corpus

La transformation du corpus en tokens correspond à la séparation des mots ou n-grammes de chaque texte. C'est ce qu'on appelle la tokénisation. Plusieurs opérations peuvent être faites à la volée.

```{r}
xyz_toks <- tokens(xyz_corp, 
                   remove_punct = TRUE,              # On supprime à la volée la ponctuation
                   remove_symbols = TRUE,            # On supprime à la volée les symboles
                   remove_numbers = FALSE,           # On pourrait supprimer à la volée les nombres
                   remove_separators = TRUE) |>      # On supprime les blancs laissés par la tokénisation
  tokens_split(separator = "'", valuetype = "fixed") # On force la tokénisation à partir de l'apostrophe

head(xyz_toks, 2)

```

#### Troisième étape: création d'un objet dfm à partir de l'objet tokens

```{r}
xyz_dfm <- dfm(xyz_toks) |>                # On transforme l'objet tokens en dfm
  dfm_remove(lsa::stopwords_fr) |>         # On élimine toutes les colonnes correspondant à un mot fonctionnel (toujours inspecter l'antidictionnaire)
  dfm_trim(min_docfreq = 0.15,             # Un mot doit être présent dans au moins 15% des documents (élimination des hapax)
           max_docfreq = 0.7,              # Un mot ne doit pas être présent dans plus de 70% des documents du corpus
           docfreq_type = "prop")           # Cet argument permet de préciser que les valeurs indiquées dans les arguments précédents sont des proportions

head(xyz_dfm, 2)

### Exercice: modifiez les seuils et voyez l'effet sur les dimensions de l'objet!
# Pour voir les dimensions de l'objet, vous n'avez qu'à l'apeller ainsi:



```


