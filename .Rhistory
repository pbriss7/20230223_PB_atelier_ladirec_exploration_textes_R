sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# Facultatif: on peut créer deux colonnes sans contenu vérifier à nouveau si R les repère
xyz$test <- NA
xyz$test2 <- ""
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# On supprime ces colonnes inutiles
xyz[, c("test", "test2")] <- NULL
# Vérification
xyz
#### Prétraitement des données ----
# Objectifs:
# 1. résoudre les problèmes que pourraient poser la manipulation et l'analyse éventuelle des données (noms de colonnes, types de données, classes d'objets, NA, encodage, etc.);
# 2. réduire les dimensions de la structure en fonction des besoins;
# 3. rendre le tableau aussi lisible que possible.
# Problème 1: les noms de colonnes contiennent des accents et des espaces.
# Problème 2: les observations ne sont pas pourvues d'identifiants uniques.
# Problème 3: certaines colonnes semblent n'apporter aucune information essentielle. À vérifier. Éliminer si inutiles.
# Problème 4: on aimerait avoir une colonne réservée aux années.
# Problème 5: on voudrait que les modalités numériques soient considérées comme des nombres, non comme des chaines de caractères.
# Problème 6: dans la colonne `texte`, les sauts de paragraphes du texte original sont indiqués par "\n".
# 1. Renommer les colonnes
colnames(xyz) <- c("periodique", "titre", "auteur",
"numero", "date", "theme", "uri",
"editeur", "issn_imp", "issn_num",
"citation", "mention_legale", "texte")
# 2. Créer un identifiant unique. On peut utiliser les numéros de ligne
xyz$doc_id <- 1:nrow(xyz)
# 3. Repérer les colonnes inutiles et les éliminer
# La première colonne semble contenir une information redondante. Vérifions:
unique(xyz[ , "periodique"])  # ou table(xyz$periodique)
unique(xyz[, "numero"])       # ou table(xyz$numero)
unique(xyz[, "theme"])        # ou table(xyz$theme)
### Exercice: poursuivez la vérification avec les colonnes `uri`, `editeur`, `issn` (x2), `mention_legale`. ###
unique(...[, ...])
strsplit(xyz$texte[1], "[")
strsplit(xyz$texte[1], "[[")
strsplit(xyz$texte[1], "[' ]")
strsplit(xyz$texte[1], "[' ]") |> unlist()
strsplit(xyz$texte[1:2], "[' ]") |> unlist()
# Application de cette fonction aux titres et aux textes
for(i in 1:nrow(xyz)){
xyz$titre_Nmots[i] <- N_mots_f(xyz$texte[i])
xyz$texte_Nmots[i] <- N_mots_f(xyz$texte[i])
}
N_mots_f <- function(x){
mots_v <- strsplit(x, "[' ]") |> unlist()
non_vide_v <- which(mots_v != "")
mots_v <- mots_v[non_vide_v]
nbre_mots <- length(mots_v)
return(nbre_mots)
}
# Application de cette fonction aux titres et aux textes
for(i in 1:nrow(xyz)){
xyz$titre_Nmots[i] <- N_mots_f(xyz$texte[i])
xyz$texte_Nmots[i] <- N_mots_f(xyz$texte[i])
}
View(xyz)
# Observons le résultat dans la table
xyz[, c("titre", "texte", "titre_Nmots", "texte_Nmots")]
# Application de cette fonction aux titres et aux textes pour créer deux nouvelles colonnes
for(i in 1:nrow(xyz)){
xyz$titre_Nmots[i] <- N_mots_f(xyz$titre[i])
xyz$texte_Nmots[i] <- N_mots_f(xyz$texte[i])
}
# Observons le résultat dans la table
xyz[, c("titre", "texte", "titre_Nmots", "texte_Nmots")]
cor.test(xyz$titre_Nmots, xyz$texte_Nmots)
# On peut projeter ces données dans un diagramme à points
ggplot(xyz, aes(x=titre_Nmots, y=texte_Nmots))+
geom_jitter()+
geom_smooth()
xyz$longueur_texte_cat <- NA                                     # Création d'une colonne remplie de NA
xyz$longueur_texte_cat
xyz$longueur_texte_cat <- factor(                                # Création de valeurs catégorielles fondées sur les modalités d'une autre colonne
ifelse(xyz$texte_Nmots < 1000, "court",
ifelse(xyz$texte_Nmots > 2500, "long", "moyen")),
levels = c("court", "moyen", "long")                           # La fonction `factor()` possède un argument, `levels=` qui permet de déterminer l'ordre des catégories
)
xyz
# On peut maintenant observer les proportions
ggplot(xyz, aes(x = longueur_texte_cat))+
geom_bar(stat = "count")
table(xyz$longueur_texte_cat)
#### Création de la matrice domifs-unifs (ou document term-matrix, dtm)
xyz_corp <- quanteda::corpus(xyz, docid_field = "doc_id", text_field = "texte")
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_separators = TRUE) |> tokens_split(separator = "'", valuetype = "fixed")
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(lsa::stopwords_fr) |> dfm_trim(min_docfreq = 0.15,
max_docfreq = 0.7,
docfreq_type = "prop")
# Ne pas oublier qu'on peut accéder aux champs informatifs n'importe quand
docvars(xyz_dfm)
#### Examen des vocabulaires en fonction d'années ou de thèmes
xyz_dfm_2015 <- dfm_subset(xyz_dfm, subset = annee == 2015)
xyz_dfm_2021 <- dfm_subset(xyz_dfm, subset = annee == 2021)
xyz_dfm_th_jardin <- dfm_subset(xyz_dfm, subset = theme == "Jardin : un enfer de morceaux de paradis")
xyz_dfm_th_YOLO <- dfm_subset(xyz_dfm, subset = theme == "YOLO (You Only Live Once) : hardis, téméraires, écervelés, aventureux, fonceurs, délurés")
# Comparaison des vocabulaires de numéros thématiques
set.seed(100)
textplot_wordcloud(xyz_dfm_th_jardin)
set.seed(100)
textplot_wordcloud(xyz_dfm_th_YOLO)
# On pourrait également comparer les vocabulaires d'auteurs ayant offert chacun 3 contributions à XYZ
set.seed(100)
dfm_subset(xyz_dfm, auteur %in% c("Edem Awumey", "J.D. Kurtness")) |>
textplot_wordcloud(comparison = TRUE)
#### Forge de ngrammes et analyse des collocations
# L'analyse des collocations est facile à faire avec quanteda.textstats
xyz_toks |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2021) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
library(readxl)               # Extension pour l'importation de fichiers Excel
library(stringr)              # Extension pour la manipulation des chaînes de caractères
library(dplyr)                # Extension pour la manipulation des structures des tableaux de données
library(ggplot2)              # Extension pour la production de graphiques de haute qualité
library(quanteda)             # Extension pour le forage textuel
library(quanteda.textplots)   # Extension pour le forage textuel
library(quanteda.textstats)   # Extension pour l'analyse des collocations
library(lsa)                  # Extension offrant un antidictionnaire élaboré
##### Importation des données ----
# Lecture des données et assignation à une variable
xyz <- readxl::read_excel("donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)
# Examen de la structure
str(xyz)
# On peut observer les détails de cette structure une information à la fois
# Nom des colonnes
colnames(xyz)
# Nombre de lignes et de colonnes dans le tableau
dim(xyz)
# Nombre de lignes
nrow(xyz)
# Regardons de quels types sont les colonnes
class(xyz$`ISSN (numérique)`)
# On veut savoir si et combien il y a des cellules sans contenu ("" ou NA)
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# Facultatif: on peut créer deux colonnes sans contenu vérifier à nouveau si R les repère
xyz$test <- NA
xyz$test2 <- ""
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# On supprime ces colonnes inutiles
xyz[, c("test", "test2")] <- NULL
# Vérification
xyz
#### Prétraitement des données ----
# Objectifs:
# 1. résoudre les problèmes que pourraient poser la manipulation et l'analyse éventuelle des données (noms de colonnes, types de données, classes d'objets, NA, encodage, etc.);
# 2. réduire les dimensions de la structure en fonction des besoins;
# 3. rendre le tableau aussi lisible que possible.
# Problème 1: les noms de colonnes contiennent des accents et des espaces.
# Problème 2: les observations ne sont pas pourvues d'identifiants uniques.
# Problème 3: certaines colonnes semblent n'apporter aucune information essentielle. À vérifier. Éliminer si inutiles.
# Problème 4: on aimerait avoir une colonne réservée aux années.
# Problème 5: on voudrait que les modalités numériques soient considérées comme des nombres, non comme des chaines de caractères.
# Problème 6: dans la colonne `texte`, les sauts de paragraphes du texte original sont indiqués par "\n".
# 1. Renommer les colonnes
colnames(xyz) <- c("periodique", "titre", "auteur",
"numero", "date", "theme", "uri",
"editeur", "issn_imp", "issn_num",
"citation", "mention_legale", "texte")
# 2. Créer un identifiant unique. On peut utiliser les numéros de ligne
xyz$doc_id <- 1:nrow(xyz)
# 3. Repérer les colonnes inutiles et les éliminer
# La première colonne semble contenir une information redondante. Vérifions:
unique(xyz[ , "periodique"])  # ou table(xyz$periodique)
unique(xyz[, "numero"])       # ou table(xyz$numero)
unique(xyz[, "theme"])        # ou table(xyz$theme)
### Exercice: poursuivez la vérification avec les colonnes `uri`, `editeur`, `issn` (x2), `mention_legale`. ###
# unique(...[, ...])
# Créons un vecteur avec les colonnes inutiles (on utilise pour cela la fonction de concaténation c( ) )
colonnes_a_supprimer <- c("periodique", "editeur", "issn_imp", "issn_num", "mention_legale", "uri", "citation")
# On élimine l'ensemble des colonnes inutiles d'un seul coup.
xyz[, colonnes_a_supprimer] <- NULL
# 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- stringr::str_extract(xyz$date, "[0-9]+")
### Exercice: éliminez la colonne `date`, qui ne sert plus à rien. ###
# xyz$... <- ...
# 5. Cette colonne `annee`, de même que la colonne `numero`, devraient être de type `numeric`, non de type `chr`.
xyz$numero <- as.integer(xyz$numero)      # La fonction as.integer() force la conversion du type chararcter en type integer
xyz$annee <- as.integer(xyz$annee)
# 6. Remplacer un symbole dans une longue chaine de caractères (un texte)
# Observons tout d'abord un texte en particulier
xyz$texte[1]
# Remplaçons par une espace simple le symbole `\n`
xyz$texte <- gsub(pattern = "\n", replacement = " ", x = xyz$texte, fixed = TRUE)
#### Exercice: vérifiez un texte pris au hasard pour voir s'il reste des scories
# xyz$...
# Enfin, par souci de lisibilité, réordonnons la séquence des colonnes
xyz <- xyz[, c("doc_id", "auteur", "titre", "numero", "annee", "theme", "texte")]
#### Exploration 1: les métadonnées ----
# La fonction `table( )` permet d'observer la fréquence des modalités d'un ou de plusieurs champs
distrib_annuelle <- table(xyz$annee)
# Quelle est la moyenne de cette distribution?
mean(distrib_annuelle)
### Exercice: créez une table pour observer la distribution des thèmes ###
# distrib_themes <- ...
# Pour ordonner cette table, on peut utiliser la fonction `sort( )`, auquel on passe l'argument `decreasing = TRUE`
distrib_themes_ord <- sort(distrib_themes, decreasing = TRUE)
# 1. Syntaxe étendue (petite perte de mémoire, mais plus explicite)
distrib_auteurs <- table(xyz$auteur)
distrib_auteurs_ord <- sort(distrib_auteurs, decreasing = TRUE)
head(distrib_auteurs_ord, n = 10)
N_mots_f <- function(x){
mots_v <- strsplit(x, "[' ]") |> unlist()
non_vide_v <- which(mots_v != "")
mots_v <- mots_v[non_vide_v]
nbre_mots <- length(mots_v)
return(nbre_mots)
}
# Application de cette fonction aux titres et aux textes pour créer deux nouvelles colonnes
for(i in 1:nrow(xyz)){
xyz$titre_Nmots[i] <- N_mots_f(xyz$titre[i])
xyz$texte_Nmots[i] <- N_mots_f(xyz$texte[i])
}
# Observons le résultat dans la table
xyz[, c("titre", "texte", "titre_Nmots", "texte_Nmots")]
cor.test(xyz$titre_Nmots, xyz$texte_Nmots)
xyz$longueur_texte_cat <- NA                                     # Création d'une colonne remplie de NA
xyz$longueur_texte_cat <- factor(                                # Création de valeurs catégorielles fondées sur les modalités d'une autre colonne
ifelse(xyz$texte_Nmots < 1000, "court",
ifelse(xyz$texte_Nmots > 2500, "long", "moyen")),
levels = c("court", "moyen", "long")                           # La fonction `factor()` possède un argument, `levels=` qui permet de déterminer l'ordre des catégories
)
# On peut maintenant observer les proportions
ggplot(xyz, aes(x = longueur_texte_cat))+
geom_bar(stat = "count")
table(xyz$longueur_texte_cat)
#### Création de la matrice domifs-unifs (ou document term-matrix, dtm)
xyz_corp <- quanteda::corpus(xyz, docid_field = "doc_id", text_field = "texte")
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_separators = TRUE) |> tokens_split(separator = "'", valuetype = "fixed")
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(lsa::stopwords_fr) |> dfm_trim(min_docfreq = 0.15,
max_docfreq = 0.7,
docfreq_type = "prop")
# Ne pas oublier qu'on peut accéder aux champs informatifs n'importe quand
docvars(xyz_dfm)
#### Examen des vocabulaires en fonction d'années ou de thèmes
xyz_dfm_2015 <- dfm_subset(xyz_dfm, subset = annee == 2015)
xyz_dfm_2021 <- dfm_subset(xyz_dfm, subset = annee == 2021)
xyz_dfm_th_jardin <- dfm_subset(xyz_dfm, subset = theme == "Jardin : un enfer de morceaux de paradis")
xyz_dfm_th_YOLO <- dfm_subset(xyz_dfm, subset = theme == "YOLO (You Only Live Once) : hardis, téméraires, écervelés, aventureux, fonceurs, délurés")
# Comparaison des vocabulaires de numéros thématiques
set.seed(100)
textplot_wordcloud(xyz_dfm_th_jardin)
set.seed(100)
textplot_wordcloud(xyz_dfm_th_YOLO)
# On pourrait également comparer les vocabulaires d'auteurs ayant offert chacun 3 contributions à XYZ
set.seed(100)
dfm_subset(xyz_dfm, auteur %in% c("Edem Awumey", "J.D. Kurtness")) |>
textplot_wordcloud(comparison = TRUE)
#### Forge de ngrammes et analyse des collocations
# L'analyse des collocations est facile à faire avec quanteda.textstats
xyz_toks |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2021) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
# On peut se servir de ce type de recherche pour ensuite créer des mots composés
xyz_toks |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2021) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
col <- xyz_toks |> tokens_remove(lsa::stopwords_fr) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
tokens_compound(xyz_toks, pattern = col) |> kwic( pattern = "jeune_fille")
#### Forge de ngrammes et analyse des collocations (association fréquente de deux mots)
# L'analyse des collocations est facile à faire avec quanteda.textstats
xyz_toks |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2021) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
# On peut se servir de ce type de recherche pour ensuite créer des mots composés
xyz_toks |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2021) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
xyz_toks |>
tokens_remove(lsa::stopwords_fr) |>
tokens_subset(annee == 2021) |>
fcm(context = "window", window = 10, tri = FALSE)
coocurrence_matrice <- xyz_toks |>
tokens_remove(lsa::stopwords_fr) |>
tokens_subset(annee == 2021) |>
fcm(context = "window", window = 10, tri = FALSE)
cooccurrence_matrice <- xyz_toks |>
tokens_remove(lsa::stopwords_fr) |>
tokens_subset(annee == 2021) |>
fcm(context = "window", window = 10, tri = FALSE)
principaux_mots <- names(topfeatures(cooccurrence_matrice, 30))
principaux_mots
set.seed(100)
textplot_network(fcm_select(cooccurrence_matrice, principaux_mots), min_freq = 0.8)
# Lecture des données et assignation à une variable
xyz <- readxl::read_excel("donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)
# Examen de la structure
str(xyz)
# On peut observer les détails de cette structure une information à la fois
# Nom des colonnes
colnames(xyz)
# Nombre de lignes et de colonnes dans le tableau
dim(xyz)
# Nombre de lignes
nrow(xyz)
# Regardons de quels types sont les colonnes
class(xyz$`ISSN (numérique)`)
# On veut savoir si et combien il y a des cellules sans contenu ("" ou NA)
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# Facultatif: on peut créer deux colonnes sans contenu vérifier à nouveau si R les repère
xyz$test <- NA
xyz$test2 <- ""
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# On supprime ces colonnes inutiles
xyz[, c("test", "test2")] <- NULL
# Vérification
xyz
colnames(xyz) <- c("periodique", "titre", "auteur",
"numero", "date", "theme", "uri",
"editeur", "issn_imp", "issn_num",
"citation", "mention_legale", "texte")
colnames(xyz)
# 2. Créer un identifiant unique. On peut utiliser les numéros de ligne
xyz$doc_id <- 1:nrow(xyz)
View(xyz)
unique(xyz[ , "periodique"])
View(xyz)
table(xyz$periodique)
unique(xyz[, "numero"])
unique(xyz[, "theme"])
colonnes_a_supprimer <- c("periodique", "editeur", "issn_imp", "issn_num", "mention_legale", "uri", "citation")
# On élimine l'ensemble des colonnes inutiles d'un seul coup.
xyz[, colonnes_a_supprimer] <- NULL
xyz$numero <- as.integer(xyz$numero)      # La fonction as.integer() force la conversion du type chararcter en type integer
xyz$annee <- as.integer(xyz$annee)
xyz
# 6. Remplacer un symbole dans une longue chaine de caractères (un texte)
# Observons tout d'abord un texte en particulier
xyz$texte[1]
# Remplaçons par une espace simple le symbole `\n`
xyz$texte <- gsub(pattern = "\n", replacement = " ", x = xyz$texte, fixed = TRUE)
# 6. Remplacer un symbole dans une longue chaine de caractères (un texte)
# Observons tout d'abord un texte en particulier
xyz$texte[1]
xyz
# Enfin, par souci de lisibilité, réordonnons la séquence des colonnes
xyz <- xyz[, c("doc_id", "auteur", "titre", "numero", "annee", "theme", "texte")]
# Problème 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- stringr::str_extract(xyz$date, "[0-9]+")
# Problème 5. Cette colonne `annee`, de même que la colonne `numero`, devraient être de type `numeric`, non de type `chr`.
xyz$numero <- as.integer(xyz$numero)      # La fonction as.integer() force la conversion du type chararcter en type integer
# 6. Remplacer un symbole dans une longue chaine de caractères (un texte)
# Observons tout d'abord un texte en particulier
xyz$texte[1]
# Remplaçons par une espace simple le symbole `\n`
xyz$texte <- gsub(pattern = "\n", replacement = " ", x = xyz$texte, fixed = TRUE)
# Enfin, par souci de lisibilité, réordonnons la séquence des colonnes
xyz <- xyz[, c("doc_id", "auteur", "titre", "numero", "annee", "theme", "texte")]
xyz
xyz$annee <- as.integer(xyz$annee)
#### Exploration 1: les métadonnées ----
# La fonction `table( )` permet d'observer la fréquence des modalités d'un ou de plusieurs champs
distrib_annuelle <- table(xyz$annee)
distrib_annuelle
# Quelle est la moyenne de cette distribution?
mean(distrib_annuelle)
distrib_auteurs <- table(xyz$auteur)
distrib_auteurs
distrib_auteurs_ord <- sort(distrib_auteurs, decreasing = TRUE)
head(distrib_auteurs_ord, n = 10)
head(sort(table(xyz$auteur), decreasing = TRUE), 10)
mtcars
# On voudrait savoir s'il y a une corrélation, positive ou négative, entre le volume du moteur (disp) et la distance/gallon que peut parcourir une voiture
plot(mtcars$disp, mtcars$mpg)
# Cette corrélation est confirmée par la p-value
cor.test(mtcars$disp, mtcars$mpg)
N_mots_f <- function(x){
mots_v <- strsplit(x, "[' ]") |> unlist()
non_vide_v <- which(mots_v != "")
mots_v <- mots_v[non_vide_v]
nbre_mots <- length(mots_v)
return(nbre_mots)
}
xyz
# Application de cette fonction aux titres et aux textes pour créer deux nouvelles colonnes
for(i in 1:nrow(xyz)){
xyz$titre_Nmots[i] <- N_mots_f(xyz$titre[i])
xyz$texte_Nmots[i] <- N_mots_f(xyz$texte[i])
}
xyz
# Observons le résultat dans la table
xyz[, c("titre", "texte", "titre_Nmots", "texte_Nmots")]
cor.test(xyz$titre_Nmots, xyz$texte_Nmots)
# On peut projeter ces données dans un diagramme à points
ggplot(xyz, aes(x=titre_Nmots, y=texte_Nmots))+
geom_jitter()+
geom_smooth()
strsplit(xyz$texte[1], "[' ]") |> unlist()
N_mots_f <- function(x){
mots_v <- strsplit(x, "[' ]") |> unlist()
non_vide_v <- which(mots_v != "")
mots_v <- mots_v[non_vide_v]
nbre_mots <- length(mots_v)
return(nbre_mots)
}
xyz$ncharTitre <- nchar(xyz$titre)
xyz
xyz$ncharTexte <- nchar(xyz$texte)
xyz
xyz$titre_Nmots <- NULL
xyz$texte_Nmots <- NULL
# Observons le résultat dans la table
xyz[, c("titre", "texte", "ncharTitre", "ncharTexte")]
cor.test(xyz$ncharTitre, xyz$ncharTexte)
# On peut projeter ces données dans un diagramme à points
ggplot(xyz, aes(x=ncharTitre, y=ncharTexte))+
geom_jitter()+
geom_smooth()
##### Exemple 2 -----
# Un numéro thématique
table(xyz$theme)
xyz$ncharTexte
summary(xyz$ncharTexte)
summary(xyz$ncharTexte)
class(summary(xyz$ncharTexte))
summary(xyz$ncharTexte)[1]
as.data.table(summary(xyz$ncharTexte))
as.data.frame(summary(xyz$ncharTexte))
as.data.frame(summary(xyz$ncharTexte))
as.data.frame(unclass(summary(xyz$ncharTexte)))
data.frame(unclass(summary(xyz$ncharTexte)))
unclass(summary(xyz$ncharTexte))
unclass(summary(xyz$ncharTexte))[1]
unclass(summary(xyz$ncharTexte))[2]
percentiles <- unclass(summary(xyz$ncharTexte))
percentiles
xyz$longueur_texte_cat <- factor(                                # Création de valeurs catégorielles fondées sur les modalités d'une autre colonne
ifelse(xyz$texte_Nmots < percentiles[2], "court",
ifelse(xyz$texte_Nmots > percentiles[5], "long", "moyen")),
levels = c("court", "moyen", "long")                           # La fonction `factor()` possède un argument, `levels=` qui permet de déterminer l'ordre des catégories
)
percentiles[5]
xyz$longueur_texte_cat <- factor(                                # Création de valeurs catégorielles fondées sur les modalités d'une autre colonne
ifelse(xyz$ncharTexte < percentiles[2], "court",
ifelse(xyz$ncharTexte > percentiles[5], "long", "moyen")),
levels = c("court", "moyen", "long")                           # La fonction `factor()` possède un argument, `levels=` qui permet de déterminer l'ordre des catégories
)
xyz
xyz$longueur_texte_cat <- factor(                                # Création de valeurs catégorielles fondées sur les modalités d'une autre colonne
ifelse(xyz$ncharTexte < percentiles["1st Qu."], "court",
ifelse(xyz$ncharTexte > percentiles["3rd Qu."], "long", "moyen")),
levels = c("court", "moyen", "long")                           # La fonction `factor()` possède un argument, `levels=` qui permet de déterminer l'ordre des catégories
)
xyz
# On peut maintenant observer les proportions
ggplot(xyz, aes(x = longueur_texte_cat))+
geom_bar(stat = "count")
table(xyz$longueur_texte_cat)
??quanteda
?quanteda::corpus()
#### Création de la matrice documents-mots
xyz_corp <- quanteda::corpus(xyz, docid_field = "doc_id", text_field = "texte")   # Les arguments de la fonction corpus() permettent de préciser les champs du tableau correspondant aux identifiants et aux textes
tokens(xyz_corp,
remove_punct = TRUE,                                           # On supprime à la volée la ponctuation
remove_symbols = TRUE,                                         # On supprime à la volée les symboles
remove_numbers = FALSE,                                        # On pourrait supprimer à la volée les nombres
remove_separators = TRUE)
tokens(xyz_corp,
remove_punct = TRUE,                                           # On supprime à la volée la ponctuation
remove_symbols = TRUE,                                         # On supprime à la volée les symboles
remove_numbers = FALSE,                                        # On pourrait supprimer à la volée les nombres
remove_separators = FALSE)
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,                                           # On supprime à la volée la ponctuation
remove_symbols = TRUE,                                         # On supprime à la volée les symboles
remove_numbers = FALSE,                                        # On pourrait supprimer à la volée les nombres
remove_separators = FALSE) |>                                  # On supprime les blancs laissés par la tokénisation
tokens_split(separator = "'", valuetype = "fixed")                              # On force la tokénisation à partir de l'apostrophe
xyz_toks
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,                                           # On supprime à la volée la ponctuation
remove_symbols = TRUE,                                         # On supprime à la volée les symboles
remove_numbers = FALSE,                                        # On pourrait supprimer à la volée les nombres
remove_separators = TRUE) |>                                  # On supprime les blancs laissés par la tokénisation
tokens_split(separator = "'", valuetype = "fixed")                              # On force la tokénisation à partir de l'apostrophe
xyz_toks
lsa::stopwords_fr
xyz_dfm <- dfm(xyz_toks) |>                                                       # On transforme l'objet tokens en dfm
dfm_remove(lsa::stopwords_fr) |>                                                # On élimine toutes les colonnes correspondant à un mot fonctionnel (toujours inspecter l'antidictionnaire)
dfm_trim(min_docfreq = 0.15,                                                    # Un mot doit être présent dans au moins 15% des documents (élimination des hapax)
max_docfreq = 0.7,                                                     # Un mot ne doit pas être présent dans plus de 70% des documents du corpus
docfreq_type = "prop")                                                 # Cet argument permet de préciser que les valeurs indiquées dans les arguments précédents sont des proportions
xyz_dfm
### Exercice: modifiez les seuils et voyez l'effet sur les dimensions de l'objet!
# Pour voir les dimensions de l'objet, vous n'avez qu'à l'apeller ainsi:
xyz_dfm
# Vous pouvez également accéder en tout temps aux métadonnées
docvars(xyz_dfm)
cooccurrence_matrice <- xyz_toks |>                                   # Envoi dans la chaîne d'opérations l'objet 'tokens' créé auparavant
tokens_remove(lsa::stopwords_fr) |>                                 # Retrait des mots fonctionnels avec l'antidictionnaire lsa
tokens_subset(annee == 2021) |>                                     # Utilisation des mots présents seulement dans les documents de l'année 2021
fcm(context = "window", window = 10, tri = FALSE)                   # On choisit ici une fenêtre contextuelle de 10 mots
# On repère ici couples de mots ayant un coefficient de cooccurrence égal ou supérieur à 30
principaux_mots <- names(topfeatures(cooccurrence_matrice, 30))
# La matrice de cooccurrence est réduite à ses principaux mots, et projetée sous la forme d'un graphique
set.seed(100)
textplot_network(fcm_select(cooccurrence_matrice, principaux_mots), min_freq = 0.8)
# La matrice de cooccurrence est réduite à ses principaux mots, et projetée sous la forme d'un graphique
set.seed(100)
textplot_network(fcm_select(cooccurrence_matrice, principaux_mots))
# La matrice de cooccurrence est réduite à ses principaux mots, et projetée sous la forme d'un graphique
set.seed(100)
textplot_network(fcm_select(cooccurrence_matrice, principaux_mots), min_freq = 0.8)
