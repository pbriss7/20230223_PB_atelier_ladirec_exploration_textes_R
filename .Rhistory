# Installation des extensions dont nous aurons besoin (si elles ne sont pas déjà installées)
if(!"stringr" %in% rownames(installed.packages())) {install.packages("stringr")}
if(!"readxl" %in% rownames(installed.packages())) {install.packages("readxl")}
if(!"dplyr" %in% rownames(installed.packages())) {install.packages("dplyr")}
if(!"ggplot2" %in% rownames(installed.packages())) {install.packages("ggplot2")}
if(!"quanteda" %in% rownames(installed.packages())) {install.packages("quanteda")}
if(!"quanteda.textplots" %in% rownames(installed.packages())) {install.packages("quanteda.textplots")}
if(!"quanteda.textstats" %in% rownames(installed.packages())) {install.packages("quanteda.textstats")}
if(!"lsa" %in% rownames(installed.packages())) {install.packages("lsa")}
# Activons les extensions (i.e. chargeons en mémoire toutes les fonctions qu'elles contiennent)
library(readxl)               # Extension pour l'importation de fichiers Excel
library(stringr)              # Extension pour la manipulation des chaînes de caractères
library(dplyr)                # Extension pour la manipulation des structures des tableaux de données
library(ggplot2)              # Extension pour la production de graphiques de haute qualité
library(quanteda)             # Extension pour le forage textuel
library(quanteda.textplots)   # Extension pour le forage textuel
library(quanteda.textstats)   # Extension pour l'analyse des collocations
library(lsa)                  # Extension offrant un antidictionnaire élaboré
##### Importation des données -----
# Lecture des données et assignation à une variable
xyz <- readxl::read_excel("donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)
##### Examen de la structure -----
str(xyz)
# On peut observer les détails de cette structure une information à la fois
# Nom des colonnes
colnames(xyz)
# Nombre de lignes et de colonnes dans le tableau
dim(xyz)
# Nombre de lignes
nrow(xyz)
# Regardons de quels types sont les colonnes
class(xyz$`ISSN (numérique)`)
# On veut savoir si et combien il y a des cellules sans contenu ("" ou NA)
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# Facultatif: on peut créer deux colonnes sans contenu vérifier à nouveau si R les repère
xyz$test <- NA
xyz$test2 <- ""
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# On supprime ces colonnes inutiles
xyz[, c("test", "test2")] <- NULL
# Vérification
xyz
#### Prétraitement des données ----
# Objectifs:
# 1. résoudre les problèmes que pourraient poser la manipulation et l'analyse éventuelle des données (noms de colonnes, types de données, classes d'objets, NA, encodage, etc.);
# 2. réduire les dimensions de l'objet en fonction des tâches à exécuter (ex.: ne conserver que les colonnes/lignes nécessaires);
# 3. rendre le tableau aussi lisible que possible.
# Problème 1: les noms de colonnes contiennent des accents et des espaces.
# Problème 2: les observations ne sont pas pourvues d'identifiants uniques.
# Problème 3: certaines colonnes semblent n'apporter aucune information essentielle. À vérifier. Éliminer si inutiles.
# Problème 4: on aimerait avoir une colonne réservée aux années.
# Problème 5: on voudrait que les valeurs numériques soient considérées comme des nombres, non comme des chaines de caractères.
# Problème 6: dans la colonne `texte`, les sauts de paragraphes du texte original sont indiqués par "\n".
##### Problème 1. Renommer les colonnes -----
colnames(xyz) <- c("periodique", "titre", "auteur",
"numero", "date", "theme", "uri",
"editeur", "issn_imp", "issn_num",
"citation", "mention_legale", "texte")
##### Problème 2. Créer un identifiant unique. On peut utiliser les numéros de ligne -----
xyz$doc_id <- 1:nrow(xyz)
##### Problème 3. Repérer les colonnes inutiles et les éliminer -----
# La première colonne semble contenir une information redondante. Vérifions:
unique(xyz[ , "periodique"])  # ou table(xyz$periodique)
unique(xyz[, "numero"])       # ou table(xyz$numero)
unique(xyz[, "theme"])        # ou table(xyz$theme)
### Exercice: poursuivez la vérification avec les colonnes `uri`, `editeur`, `issn` (x2), `mention_legale`. ###
# unique(...[, ...])
# Créons un vecteur avec les colonnes inutiles (on utilise pour cela la fonction de concaténation c( ) )
colonnes_a_supprimer <- c("periodique", "editeur", "issn_imp", "issn_num", "mention_legale", "uri", "citation")
# On élimine l'ensemble des colonnes inutiles d'un seul coup.
xyz[, colonnes_a_supprimer] <- NULL
# Problème 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- stringr::str_extract(xyz$date, "[0-9]+")
### Exercice: éliminez la colonne `date`, qui ne sert plus à rien. ###
# xyz$... <- ...
##### Problème 5. Cette colonne `annee`, de même que la colonne `numero`, devraient être de type `numeric`, non de type `chr` -----
xyz$numero <- as.integer(xyz$numero)      # La fonction as.integer() force la conversion du type chararcter en type integer
# xyz$... <- ...(xyz$...)
##### 6. Remplacer un symbole dans une longue chaine de caractères (un texte) -----
# Observons tout d'abord un texte en particulier
xyz$texte[1]
# Remplaçons par une espace simple le symbole `\n`
xyz$texte <- gsub(pattern = "\n", replacement = " ", x = xyz$texte, fixed = TRUE)
#### Exercice: vérifiez un texte pris au hasard pour voir s'il reste des scories
# xyz$...
# Enfin, par souci de lisibilité, réordonnons la séquence des colonnes
xyz <- xyz[, c("doc_id", "auteur", "titre", "numero", "annee", "theme", "texte")]
#### EXPLORATION 1: LES MÉTADONNÉES ----
# La fonction `table( )` permet d'observer la fréquence des modalités d'un ou de plusieurs champs
distrib_annuelle <- table(xyz$annee)
# Quelle est la moyenne de cette distribution?
mean(distrib_annuelle)
### Exercice: créez une table pour observer la distribution des thèmes ###
# distrib_themes <- ...
# Pour ordonner cette table, on peut utiliser la fonction `sort( )`, auquel on passe l'argument `decreasing = TRUE`
# distrib_themes_ord <- sort(distrib_themes, decreasing = TRUE)
### Exercice: trouvez les noms des 10 principaux contributeurs de la revue ###
# distrib_auteurs <- ...
# distrib_auteurs_ord <- ...
# Réponse à la question ci-dessus (j'en profite pour vous montrer trois syntaxes différentes produisant le même résultat):
# 1. Syntaxe étendue (petite perte de mémoire, mais plus explicite: le résultat de chaque opération est emmagasiné dans une variable)
distrib_auteurs <- table(xyz$auteur)
distrib_auteurs_ord <- sort(distrib_auteurs, decreasing = TRUE)
head(distrib_auteurs_ord, n = 10)
# 2. Syntaxe condensée, par enchâssement des fonctions. Lire de l'intérieur vers l'extérieur
head(sort(table(xyz$auteur), decreasing = TRUE), 10)
# 3. Syntaxe enchainée (pipe), qui se lit de gauche à droite
xyz$auteur |> table() |> sort(decreasing = TRUE) |> head(10)
##### Tests de corrélation -----
# Une tâche importante dans l'AED est de comprendre s'il existe des corrélations entre des variables
# Les corrélations se calculent sur des variables numériques
# EXEMPLE avec un jeu de données disponible en tout temps dans l'extension de base, `mtcars`
# Ce jeu de données présente différentes marques et modèles de voitures et leurs attributs techniques
# Il permet notamment de connaître la distance au "gallon" parcourue par différentes marques selon le volume de leurs moteurs
mtcars # Dans le jeu de données, les noms de lignes (rownames) correspondent aux marques.
# On voudrait savoir s'il y a une corrélation, positive ou négative, entre le volume du moteur (disp) et la distance/gallon que peut parcourir une voiture
# Expliquer variable indépendante / dépendante
plot(mtcars$disp, mtcars$mpg)
# Cette corrélation est confirmée par la p-value
# Ajouter quelques informations sur la corrélation - et +, sur la force de la corrélation et la p-value
cor.test(mtcars$disp, mtcars$mpg)
xyz$ncharTitre <- nchar(xyz$titre)
xyz$ncharTexte <- nchar(xyz$texte)
cor.test(xyz$ncharTitre, xyz$ncharTexte)
cor.test(xyz$ncharTitre, xyz$ncharTexte, method = "kendall")
cor.test(xyz$ncharTitre, xyz$ncharTexte, method = "kendall")
if(!"textrank" %in% rownames(installed.packages())){install.packages("textrank")}
library(textrank)
?cooccurrence
library(lattice)
# Activer les extensions
libs <- c( "stringr", "tidytext", "lsa", "udpipe", "data.table", "textrank", "gutenbergr")
lapply(libs, require, character.only = TRUE)
if(!"udpipe" %in% rownames(installed.packages())){install.packages("udpipe")}
if(!"data.table" %in% rownames(installed.packages())){install.packages("data.table")}
if(!"stringr" %in% rownames(installed.packages())){install.packages("stringr")}
if(!"textrank" %in% rownames(installed.packages())){install.packages("textrank")}
# Activer les extensions
libs <- c( "stringr", "tidytext", "lsa", "udpipe", "data.table", "textrank", "gutenbergr")
lapply(libs, require, character.only = TRUE)
rm(list = ls())
# Création du chemin du modèle d'annotation
model_dir <- paste0(getwd(), '/modele_udpipe')
# Télécharger le modèle d'annotation
udtarget <- udpipe_download_model(language = "french-gsd", model_dir = model_dir)
# Importer dans l'environnement de travail le modèle d'annotation téléchargé
udmodel <- udpipe_load_model(file = udtarget$file_model)
# Importation du texte de Maria Chapdelaine avec son identifiant unique
maria <- gutenberg_download(13525, mirror = "http://mirror.csclub.uwaterloo.ca/gutenberg/")
# Transformation de l'encodage des chaînes de caractères du vecteur text
maria$text <- iconv(maria$text, from = "latin1", "utf8")
# Élimination des lignes blanches
maria <- maria[maria$text != "", ]
# Inspection des 10 premières lignes
maria[1:10, ]
# Élimination du péritexte
maria <- maria[grep("CHAPITRE I\\b", maria$text):nrow(maria), ]
# On s'assure que la fin du roman coïncide bien avec la dernière ligne du document
maria[(nrow(maria)-10):nrow(maria), ]
# Assemblage du texte en un vecteur de longueur 1
maria_v <- paste(maria$text, collapse = " ")
x <- udpipe_annotate(udmodel,
maria_v,
doc_id = "maria_chapdelaine",
parallel.cores = 10
) %>% as.data.table(.) %>%
subset(., !is.na(upos)) # Élimination des éléments que le modèle n'a pu annoter
stats <- subset(x, upos %in% "NOUN")
stats <- txt_freq(x = stats$lemma)
library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 30), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")
stats
library(lattice)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 30), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")
barchart(key ~ freq, data = head(stats, 30), col = "cadetblue", main = "Most occurring nouns", xlab = "Freq")
## Collocation (words following one another)
stats <- keywords_collocation(x = x,
term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
ngram_max = 4)
stats
## Co-occurrences: How frequent do words occur in the same sentence, in this case only nouns or adjectives
stats <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")),
term = "lemma", group = c("doc_id", "paragraph_id", "sentence_id"))
stats
## Co-occurrences: How frequent do words follow one another
stats <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"))
stats
## Co-occurrences: How frequent do words follow one another even if we would skip 2 words in between
stats <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"), skipgram = 2)
head(stats)
library(igraph)
library(ggraph)
library(ggplot2)
wordnetwork <- head(stats, 30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
theme_graph(base_family = "Arial Narrow") +
theme(legend.position = "none") +
labs(title = "Cooccurrences within 3 words distance", subtitle = "Nouns & Adjective")
## Simple noun phrases (a adjective+noun, pre/postposition, optional determiner and another adjective+noun)
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
stats <- keywords_phrases(x = x$phrase_tag, term = x$token,
pattern = "(A|N)+N(P+D*(A|N)*N)*",
is_regex = TRUE, ngram_max = 4, detailed = FALSE)
head(subset(stats, ngram > 2))
stats <- merge(x, x,
by.x = c("doc_id", "paragraph_id", "sentence_id", "head_token_id"),
by.y = c("doc_id", "paragraph_id", "sentence_id", "token_id"),
all.x = TRUE, all.y = FALSE,
suffixes = c("", "_parent"), sort = FALSE)
stats <- subset(stats, dep_rel %in% "nsubj" & upos %in% c("NOUN") & upos_parent %in% c("ADJ"))
stats$term <- paste(stats$lemma_parent, stats$lemma, sep = " ")
stats <- txt_freq(stats$term)
library(wordcloud)
wordcloud(words = stats$key, freq = stats$freq, min.freq = 3, max.words = 100,
random.order = FALSE, colors = c("#1B9E77", "#D95F02", "#7570B3", "#E7298A", "#66A61E", "#E6AB02"))
stats <- textrank_keywords(x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"),
ngram_max = 8, sep = " ")
stats <- subset(stats$keywords, ngram > 1 & freq >= 3)
library(wordcloud)
wordcloud(words = stats$keyword, freq = stats$freq)
stats <- keywords_rake(x = x,
term = "token", group = c("doc_id", "paragraph_id", "sentence_id"),
relevant = x$upos %in% c("NOUN", "ADJ"),
ngram_max = 4)
head(subset(stats, freq > 3))
cooccurrence_matrice <- xyz_toks |>                                   # Envoi dans la chaîne d'opérations l'objet 'tokens' créé auparavant
tokens_remove(lsa::stopwords_fr) |>                                 # Retrait des mots fonctionnels avec l'antidictionnaire lsa
tokens_subset(annee == 2021) |>                                     # Utilisation des mots présents seulement dans les documents de l'année 2021
fcm(context = "window", window = 10, tri = FALSE)                   # On choisit ici une fenêtre contextuelle de 10 mots
#### Création de la matrice documents-mots ----
xyz_corp <- quanteda::corpus(xyz, docid_field = "doc_id", text_field = "texte")   # Les arguments de la fonction corpus() permettent de préciser les champs du tableau correspondant aux identifiants et aux textes
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,                                           # On supprime à la volée la ponctuation
remove_symbols = TRUE,                                         # On supprime à la volée les symboles
remove_numbers = FALSE,                                        # On pourrait supprimer à la volée les nombres
remove_separators = TRUE) |>                                   # On supprime les blancs laissés par la tokénisation
tokens_split(separator = "'", valuetype = "fixed")                              # On force la tokénisation à partir de l'apostrophe
xyz_dfm <- dfm(xyz_toks) |>                                                       # On transforme l'objet tokens en dfm
dfm_remove(lsa::stopwords_fr) |>                                                # On élimine toutes les colonnes correspondant à un mot fonctionnel (toujours inspecter l'antidictionnaire)
dfm_trim(min_docfreq = 0.15,                                                    # Un mot doit être présent dans au moins 15% des documents (élimination des hapax)
max_docfreq = 0.7,                                                     # Un mot ne doit pas être présent dans plus de 70% des documents du corpus
docfreq_type = "prop")                                                 # Cet argument permet de préciser que les valeurs indiquées dans les arguments précédents sont des proportions
#### Préparation de l'environnement de travail ----
# Installation des extensions dont nous aurons besoin (si elles ne sont pas déjà installées)
if(!"stringr" %in% rownames(installed.packages())) {install.packages("stringr")}
if(!"readxl" %in% rownames(installed.packages())) {install.packages("readxl")}
if(!"dplyr" %in% rownames(installed.packages())) {install.packages("dplyr")}
if(!"ggplot2" %in% rownames(installed.packages())) {install.packages("ggplot2")}
if(!"quanteda" %in% rownames(installed.packages())) {install.packages("quanteda")}
if(!"quanteda.textplots" %in% rownames(installed.packages())) {install.packages("quanteda.textplots")}
if(!"quanteda.textstats" %in% rownames(installed.packages())) {install.packages("quanteda.textstats")}
if(!"lsa" %in% rownames(installed.packages())) {install.packages("lsa")}
# Activons les extensions (i.e. chargeons en mémoire toutes les fonctions qu'elles contiennent)
library(readxl)               # Extension pour l'importation de fichiers Excel
library(stringr)              # Extension pour la manipulation des chaînes de caractères
library(dplyr)                # Extension pour la manipulation des structures des tableaux de données
library(ggplot2)              # Extension pour la production de graphiques de haute qualité
library(quanteda)             # Extension pour le forage textuel
library(quanteda.textplots)   # Extension pour le forage textuel
library(quanteda.textstats)   # Extension pour l'analyse des collocations
library(lsa)                  # Extension offrant un antidictionnaire élaboré
##### Importation des données -----
# Lecture des données et assignation à une variable
xyz <- readxl::read_excel("donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)
##### Examen de la structure -----
str(xyz)
# On peut observer les détails de cette structure une information à la fois
# Nom des colonnes
colnames(xyz)
# Nombre de lignes et de colonnes dans le tableau
dim(xyz)
# Nombre de lignes
nrow(xyz)
# Regardons de quels types sont les colonnes
class(xyz$`ISSN (numérique)`)
# On veut savoir si et combien il y a des cellules sans contenu ("" ou NA)
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# Facultatif: on peut créer deux colonnes sans contenu vérifier à nouveau si R les repère
xyz$test <- NA
xyz$test2 <- ""
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# On supprime ces colonnes inutiles
xyz[, c("test", "test2")] <- NULL
# Vérification
xyz
#### Prétraitement des données ----
# Objectifs:
# 1. résoudre les problèmes que pourraient poser la manipulation et l'analyse éventuelle des données (noms de colonnes, types de données, classes d'objets, NA, encodage, etc.);
# 2. réduire les dimensions de l'objet en fonction des tâches à exécuter (ex.: ne conserver que les colonnes/lignes nécessaires);
# 3. rendre le tableau aussi lisible que possible.
# Problème 1: les noms de colonnes contiennent des accents et des espaces.
# Problème 2: les observations ne sont pas pourvues d'identifiants uniques.
# Problème 3: certaines colonnes semblent n'apporter aucune information essentielle. À vérifier. Éliminer si inutiles.
# Problème 4: on aimerait avoir une colonne réservée aux années.
# Problème 5: on voudrait que les valeurs numériques soient considérées comme des nombres, non comme des chaines de caractères.
# Problème 6: dans la colonne `texte`, les sauts de paragraphes du texte original sont indiqués par "\n".
##### Problème 1. Renommer les colonnes -----
colnames(xyz) <- c("periodique", "titre", "auteur",
"numero", "date", "theme", "uri",
"editeur", "issn_imp", "issn_num",
"citation", "mention_legale", "texte")
##### Problème 2. Créer un identifiant unique. On peut utiliser les numéros de ligne -----
xyz$doc_id <- 1:nrow(xyz)
##### Problème 3. Repérer les colonnes inutiles et les éliminer -----
# La première colonne semble contenir une information redondante. Vérifions:
unique(xyz[ , "periodique"])  # ou table(xyz$periodique)
unique(xyz[, "numero"])       # ou table(xyz$numero)
unique(xyz[, "theme"])        # ou table(xyz$theme)
### Exercice: poursuivez la vérification avec les colonnes `uri`, `editeur`, `issn` (x2), `mention_legale`. ###
# unique(...[, ...])
# Créons un vecteur avec les colonnes inutiles (on utilise pour cela la fonction de concaténation c( ) )
colonnes_a_supprimer <- c("periodique", "editeur", "issn_imp", "issn_num", "mention_legale", "uri", "citation")
# On élimine l'ensemble des colonnes inutiles d'un seul coup.
xyz[, colonnes_a_supprimer] <- NULL
# Problème 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- stringr::str_extract(xyz$date, "[0-9]+")
### Exercice: éliminez la colonne `date`, qui ne sert plus à rien. ###
# xyz$... <- ...
##### Problème 5. Cette colonne `annee`, de même que la colonne `numero`, devraient être de type `numeric`, non de type `chr` -----
xyz$numero <- as.integer(xyz$numero)      # La fonction as.integer() force la conversion du type chararcter en type integer
# xyz$... <- ...(xyz$...)
##### 6. Remplacer un symbole dans une longue chaine de caractères (un texte) -----
# Observons tout d'abord un texte en particulier
xyz$texte[1]
# Remplaçons par une espace simple le symbole `\n`
xyz$texte <- gsub(pattern = "\n", replacement = " ", x = xyz$texte, fixed = TRUE)
#### Exercice: vérifiez un texte pris au hasard pour voir s'il reste des scories
# xyz$...
# Enfin, par souci de lisibilité, réordonnons la séquence des colonnes
xyz <- xyz[, c("doc_id", "auteur", "titre", "numero", "annee", "theme", "texte")]
#### EXPLORATION 1: LES MÉTADONNÉES ----
# La fonction `table( )` permet d'observer la fréquence des modalités d'un ou de plusieurs champs
distrib_annuelle <- table(xyz$annee)
# Quelle est la moyenne de cette distribution?
mean(distrib_annuelle)
### Exercice: créez une table pour observer la distribution des thèmes ###
# distrib_themes <- ...
# Pour ordonner cette table, on peut utiliser la fonction `sort( )`, auquel on passe l'argument `decreasing = TRUE`
# distrib_themes_ord <- sort(distrib_themes, decreasing = TRUE)
### Exercice: trouvez les noms des 10 principaux contributeurs de la revue ###
# distrib_auteurs <- ...
# distrib_auteurs_ord <- ...
# Réponse à la question ci-dessus (j'en profite pour vous montrer trois syntaxes différentes produisant le même résultat):
# 1. Syntaxe étendue (petite perte de mémoire, mais plus explicite: le résultat de chaque opération est emmagasiné dans une variable)
distrib_auteurs <- table(xyz$auteur)
distrib_auteurs_ord <- sort(distrib_auteurs, decreasing = TRUE)
head(distrib_auteurs_ord, n = 10)
# 2. Syntaxe condensée, par enchâssement des fonctions. Lire de l'intérieur vers l'extérieur
head(sort(table(xyz$auteur), decreasing = TRUE), 10)
# 3. Syntaxe enchainée (pipe), qui se lit de gauche à droite
xyz$auteur |> table() |> sort(decreasing = TRUE) |> head(10)
##### Tests de corrélation -----
# Une tâche importante dans l'AED est de comprendre s'il existe des corrélations entre des variables
# Les corrélations se calculent sur des variables numériques
# EXEMPLE avec un jeu de données disponible en tout temps dans l'extension de base, `mtcars`
# Ce jeu de données présente différentes marques et modèles de voitures et leurs attributs techniques
# Il permet notamment de connaître la distance au "gallon" parcourue par différentes marques selon le volume de leurs moteurs
mtcars # Dans le jeu de données, les noms de lignes (rownames) correspondent aux marques.
# On voudrait savoir s'il y a une corrélation, positive ou négative, entre le volume du moteur (disp) et la distance/gallon que peut parcourir une voiture
# Expliquer variable indépendante / dépendante
plot(mtcars$disp, mtcars$mpg)
# Cette corrélation est confirmée par la p-value
# Ajouter quelques informations sur la corrélation - et +, sur la force de la corrélation et la p-value
cor.test(mtcars$disp, mtcars$mpg)
# Pour effectuer des tests de corrélation sur des données textuelles, il faut au préalable se demander ce qu'on souhaite mesurer et générer des données numériques qui serviront au calcul.
##### Exemple -----
# On pourrait vouloir vérifier s'il y a une corrélation, positive ou négative, entre la longueur des titres et la longueur des textes.
# Le test de corrélation est un test statistique qui mesure l'interdépendance ou l'association entre des paires de valeurs (deux variables).
# Nous allons utiliser le coefficient de corrélation appelé tau de Kendall (compris entre -1 et +1).
# Une corrélation positive est marquée par un tau positif, et inversement pour une corrélation négative. Plus le nombre s'éloigne de 0, plus la corrélation est forte.
# Référence: https://academic.oup.com/biomet/article/30/1-2/81/176907
# Pour faire ce test, il faut créer deux variables numériques qui seront mises en relation.
# Pour aller au plus simple, nous allons calculer le nombre de caractères des titres (première variable) et le nombre de caractères des textes (2e variable).
# Cette opération sera faite avec la fonction nchar().
xyz$ncharTitre <- nchar(xyz$titre)
xyz$ncharTexte <- nchar(xyz$texte)
# Observons le résultat dans la table
xyz[, c("titre", "texte", "ncharTitre", "ncharTexte")]
cor.test(xyz$ncharTitre, xyz$ncharTexte, method = "kendall")
# La mesure de corrélation, `tau`, est positive, mais très proche de zéro, ce qui dénote une corrélation très faible.
# On peut projeter ces données dans un diagramme à points
ggplot(xyz, aes(x=ncharTitre, y=ncharTexte))+
geom_jitter()+
geom_smooth()
#### Enrichissement des données ----
# On peut tirer profit des données numériques ajoutées (longueur des textes) pour créer un nouveau champ
# Par exemple, on pourrait souhaiter classer les textes selon qu'ils sont "courts", "moyens" ou "longs"
# Le type d'objet que nous créerons est dit "catégorique" (ou 'factor')
percentiles <- unclass(summary(xyz$ncharTexte))                  # On transforme le sommaire statistique, un objet de type `table`, en simple vecteur avec unclass()
xyz$longueur_texte_cat <- factor(                                # Création de valeurs catégorielles fondées sur les modalités d'une autre colonne
ifelse(xyz$ncharTexte < percentiles["1st Qu."], "court",
ifelse(xyz$ncharTexte > percentiles["3rd Qu."], "long", "moyen")),
levels = c("court", "moyen", "long")                           # La fonction `factor()` possède un argument, `levels=` qui permet de déterminer l'ordre des catégories
)
# On peut maintenant observer les proportions
ggplot(xyz, aes(x = longueur_texte_cat))+
geom_bar(stat = "count")
table(xyz$longueur_texte_cat)
#### EXPLORATION 2 -- LES DONNÉES TEXTUELLES ----
# Pour aller plus loin dans l'analyse des données textuelles, nous devrons transformer le tableau de données en matrice documents-mots
# Une matrice est un objet qui ressemble beaucoup à un tableau de données, à ceci près qu'il n'est composé que d'un seul type de données
# Chaque ligne de notre matrice représentera un document et chaque colonne, un mot unique du vocabulaire complet de tous les textes assemblés en "corpus"
# Ce type de matrices peut être pondérée pour accorder plus de poids à certains mots, elle peut être normalisée pour éviter les biais de longueur
# De plus, les matrices permettent des calculs vectoriels, donc très rapides.
# Pour transformer notre tableau en matrice documents-mots (ou 'document-feature matrix'), nous allons utiliser l'extension Quanteda
# Quanteda offre une multitude de fonctions utiles pour l'ADT. Sa syntaxe est cohérente et la documentation est abondante.
# Si on ne veut pas avoir à écrire ses propres fonctions (de tokénisation, de transformation, etc.), Quanteda est un bon compromis
# La transformation se fait en trois grandes étapes, chacune visant un objectif précis et permettant de faire des opérations à la volée
# La première étape est la création d'un corpus avec la fonction corpus(). On y précise la colonne des identifiants uniques et la colonne contenant le texte à analyser
# La deuxième étape est la tokénisation des textes du corpus. On peut à cette étape se servir d'un antidictionnaire pour éliminer les mots fonctionnels
# La troisième étape est le passage entre cet objet "tokens" à la dfm (document-feature matrix).
# Les métadonnées suivent par défaut chacune des transformations et permettront, une fois la dfm construite, de filtrer les documents en ligne avec les métadonnées
# Pour plus d'information sur chacune des fonctions, consultez la documentation. Exemple:
#### Création de la matrice documents-mots ----
xyz_corp <- quanteda::corpus(xyz, docid_field = "doc_id", text_field = "texte")   # Les arguments de la fonction corpus() permettent de préciser les champs du tableau correspondant aux identifiants et aux textes
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,                                           # On supprime à la volée la ponctuation
remove_symbols = TRUE,                                         # On supprime à la volée les symboles
remove_numbers = FALSE,                                        # On pourrait supprimer à la volée les nombres
remove_separators = TRUE) |>                                   # On supprime les blancs laissés par la tokénisation
tokens_split(separator = "'", valuetype = "fixed")                              # On force la tokénisation à partir de l'apostrophe
xyz_dfm <- dfm(xyz_toks) |>                                                       # On transforme l'objet tokens en dfm
dfm_remove(lsa::stopwords_fr) |>                                                # On élimine toutes les colonnes correspondant à un mot fonctionnel (toujours inspecter l'antidictionnaire)
dfm_trim(min_docfreq = 0.15,                                                    # Un mot doit être présent dans au moins 15% des documents (élimination des hapax)
max_docfreq = 0.7,                                                     # Un mot ne doit pas être présent dans plus de 70% des documents du corpus
docfreq_type = "prop")                                                 # Cet argument permet de préciser que les valeurs indiquées dans les arguments précédents sont des proportions
### Exercice: modifiez les seuils et voyez l'effet sur les dimensions de l'objet!
# Pour voir les dimensions de l'objet, vous n'avez qu'à l'apeller ainsi:
xyz_dfm
# Vous pouvez également accéder en tout temps aux métadonnées
docvars(xyz_dfm)
#### Exploration du vocabulaire ----
##### Examen des vocabulaires en fonction d'années ou de thèmes -----
xyz_dfm_2015 <- dfm_subset(xyz_dfm, subset = annee == 2015)
xyz_dfm_2021 <- dfm_subset(xyz_dfm, subset = annee == 2021)
xyz_dfm_th_jardin <- dfm_subset(xyz_dfm, subset = theme == "Jardin : un enfer de morceaux de paradis")
xyz_dfm_th_YOLO <- dfm_subset(xyz_dfm, subset = theme == "YOLO (You Only Live Once) : hardis, téméraires, écervelés, aventureux, fonceurs, délurés")
# Comparaison des vocabulaires de numéros thématiques
set.seed(100)
textplot_wordcloud(xyz_dfm_th_jardin)
set.seed(100)
textplot_wordcloud(xyz_dfm_th_YOLO)
# On pourrait également comparer les vocabulaires d'auteurs ayant offert chacun 3 contributions à XYZ
set.seed(100)
dfm_subset(xyz_dfm, auteur %in% c("Edem Awumey", "J.D. Kurtness")) |>
textplot_wordcloud(comparison = TRUE)
#### Exploration des données textuelles avec R - 23 février 2023 ----
## Note importante
# Une ligne précédée d'un ou de plusieurs croisillons # ne sera pas exécutée par l'interpréteur
# Pour exécuter une instruction, placez votre curseur au début de la ligne et appuyez sur 'COMMAND' + 'RETURN' (raccourci Mac) ou 'CTRL' + 'RETURN' (raccourci Windows)
# Pour obtenir de l'aide sur une fonction, exécutez-la précédée d'un point d'interrogation. Par exemple:
?paste()
#### Préparation de l'environnement de travail ----
# Installation des extensions dont nous aurons besoin (si elles ne sont pas déjà installées)
if(!"stringr" %in% rownames(installed.packages())) {install.packages("stringr")}
if(!"readxl" %in% rownames(installed.packages())) {install.packages("readxl")}
if(!"dplyr" %in% rownames(installed.packages())) {install.packages("dplyr")}
if(!"ggplot2" %in% rownames(installed.packages())) {install.packages("ggplot2")}
if(!"quanteda" %in% rownames(installed.packages())) {install.packages("quanteda")}
if(!"quanteda.textplots" %in% rownames(installed.packages())) {install.packages("quanteda.textplots")}
if(!"quanteda.textstats" %in% rownames(installed.packages())) {install.packages("quanteda.textstats")}
if(!"lsa" %in% rownames(installed.packages())) {install.packages("lsa")}
# Activons les extensions (i.e. chargeons en mémoire toutes les fonctions qu'elles contiennent)
library(readxl)               # Extension pour l'importation de fichiers Excel
library(stringr)              # Extension pour la manipulation des chaînes de caractères
library(dplyr)                # Extension pour la manipulation des structures des tableaux de données
library(ggplot2)              # Extension pour la production de graphiques de haute qualité
library(quanteda)             # Extension pour le forage textuel
library(quanteda.textplots)   # Extension pour le forage textuel
library(quanteda.textstats)   # Extension pour l'analyse des collocations
library(lsa)                  # Extension offrant un antidictionnaire élaboré
##### Importation des données -----
# Lecture des données et assignation à une variable
xyz <- readxl::read_excel("donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)
##### Examen de la structure -----
str(xyz)
# On peut observer les détails de cette structure une information à la fois
# Nom des colonnes
colnames(xyz)
# Nombre de lignes et de colonnes dans le tableau
dim(xyz)
# Nombre de lignes
nrow(xyz)
# Regardons de quel type sont
class(xyz$`ISSN (numérique)`)
# On veut savoir si et combien il y a des cellules sans contenu ("" ou NA)
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# Facultatif: on peut créer deux colonnes sans contenu, puis vérifier à nouveau si R repère ces valeurs nulles
xyz$test <- NA
xyz$test2 <- ""
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# On supprime ces colonnes inutiles
xyz[, c("test", "test2")] <- NULL
# Vérification
xyz
# Vérification que les colonnes ont bien disparu
colnames(xyz)
xyz$date
