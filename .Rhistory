class(xyz_toks)
attributes(xyz_toks)
lapply(xyz_toks[1:10], strsplit, "[']")
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_hyphens = TRUE,
remove_separators = TRUE) |> tokens_replace("[']", " ")
xyz_toks
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_hyphens = TRUE,
remove_separators = TRUE) |> tokens_replace(pattern = "\\'", replacement = " ")
xyz_toks
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_separators = TRUE) |> tokens_replace(pattern = "\\'", replacement = " ", valuetype = "regex")
xyz_toks
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_separators = TRUE) |> tokens_replace(pattern = "'", replacement = " ", valuetype = "fixed")
xyz_toks
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_separators = TRUE) |> tokens_replace(pattern = "'", replacement = " ", valuetype = "regex")
xyz_toks
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_separators = TRUE) |> tokens_remove("'", padding = TRUE)
xyz_toks
xyz_toks <- tokens(xyz_corp,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = FALSE,
remove_separators = TRUE) |> tokens_split(separator = "'", valuetype = "fixed")
xyz_toks
xyz_dfm <- dfm(xyz_toks, remove = stopwords(language = "fr"))
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr"))
xyz_dfm
dict <- dictionary(list(Montréal = c("[Mm]ontréal", "[Mm]ont-?Royal", "métropole"),
Québec = c("Québec", "[Cc]apitale nationale")))
dfm_lookup(xyz_dfm, dict)
sum(dfm_lookup(xyz_dfm, dict))
apply(xyz_dfm, 2, dfm_lookup, dict)
apply(xyz_dfm, 1, dfm_lookup, dict)
dfm_lookup(xyz_dfm) dict)
dfm_lookup(xyz_dfm, dict)
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr")) |> dfm_trim(min_termfreq = 0.1,
max_termfreq = 0.5,
termfreq_type = "prop",
min_docfreq = NULL,
max_docfreq = NULL,
docfreq_type = c("count", "prop", "rank", "quantile"),)
xyz_dfm
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr")) |> dfm_trim(min_termfreq = 0.01,
max_termfreq = 0.05,
termfreq_type = "prop",
min_docfreq = NULL,
max_docfreq = NULL,
docfreq_type = c("count", "prop", "rank", "quantile"),)
xyz_dfm
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr")) |> dfm_trim(min_docfreq = 0.05,
max_docfreq = 0.9,
docfreq_type = "prop")
xyz_dfm
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr")) |> dfm_trim(min_docfreq = 0.05,
max_docfreq = 0.8,
docfreq_type = "prop")
xyz_dfm
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr")) |> dfm_trim(min_docfreq = 0.1,
max_docfreq = 0.8,
docfreq_type = "prop")
xyz_dfm
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr")) |> dfm_trim(min_docfreq = 0.2,
max_docfreq = 0.8,
docfreq_type = "prop")
xyz_dfm
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr")) |> dfm_trim(min_docfreq = 0.15,
max_docfreq = 0.8,
docfreq_type = "prop")
textplot_wordcloud(xyz_dfm)
set.seed(100)
textplot_wordcloud(xyz_dfm)
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(stopwords(language = "fr")) |> dfm_trim(min_docfreq = 0.15,
max_docfreq = 0.7,
docfreq_type = "prop")
set.seed(100)
textplot_wordcloud(xyz_dfm)
xyz_dfm
dfm_subset(xyz_dfm, subset = annee == 2015)
xyz_dfm_2015 <- dfm_subset(xyz_dfm, subset = annee == 2015)
xyz_dfm_2015 <- dfm_subset(xyz_dfm, subset = annee == 2015)
xyz_dfm_2021 <- dfm_subset(xyz_dfm, subset = annee == 2021)
xyz_dfm_2015
xyz_dfm_2021
textplot_wordcloud(xyz_dfm_2015)
textplot_wordcloud(xyz_dfm_2021)
if(!"lsa" %in% rownames(installed.packages())) {install.packages("lsa")}
library(lsa)                  # Extension offrant un antidictionnaire élaboré
xyz_dfm <- dfm(xyz_toks) |> dfm_remove(lsa::stopwords_fr) |> dfm_trim(min_docfreq = 0.15,
max_docfreq = 0.7,
docfreq_type = "prop")
set.seed(100)
xyz_dfm_2015 <- dfm_subset(xyz_dfm, subset = annee == 2015)
xyz_dfm_2021 <- dfm_subset(xyz_dfm, subset = annee == 2021)
textplot_wordcloud(xyz_dfm_2021)
xyz_dfm_2015 <- dfm_subset(xyz_dfm, subset = annee == 2015)
textplot_wordcloud(xyz_dfm_2015)
docvars(xyz_dfm)
xyz_dfm_th_jardin <- dfm_subset(xyz_dfm, subset = theme == "Jardin : un enfer de morceaux de paradis")
xyz_dfm_th_jardin
xyz_dfm_th_YOLO <- dfm_subset(xyz_dfm, subset = theme == "YOLO (You Only Live Once) : hardis, téméraires, écervelés, aventureux, fonceurs, délurés")
textplot_wordcloud(xyz_dfm_th_jardin)
set.seed(100)
set.seed(100)
textplot_wordcloud(xyz_dfm_2015)
set.seed(100)
textplot_wordcloud(xyz_dfm_2021)
textplot_wordcloud(xyz_dfm_th_jardin)
set.seed(100)
textplot_wordcloud(xyz_dfm_th_jardin)
set.seed(100)
textplot_wordcloud(xyz_dfm_th_jardin)
set.seed(100)
textplot_wordcloud(xyz_dfm_th_YOLO)
set.seed(100)
textplot_wordcloud(xyz_dfm_th_jardin)
# Ne pas oublier qu'on peut accéder aux champs informatifs n'importe quand
docvars(xyz_dfm[1])
# Ne pas oublier qu'on peut accéder aux champs informatifs n'importe quand
docvars(xyz_dfm)
# Ne pas oublier qu'on peut accéder aux champs informatifs n'importe quand
docvars(xyz_dfm)[1]
# Ne pas oublier qu'on peut accéder aux champs informatifs n'importe quand
docvars(xyz_dfm)[1,1]
# Ne pas oublier qu'on peut accéder aux champs informatifs n'importe quand
docvars(xyz_dfm)[,1]
# Ne pas oublier qu'on peut accéder aux champs informatifs n'importe quand
docvars(xyz_dfm)[1,]
View(xyz_dfm)
head(auteurs_table_ord, n = 10)
# On pourrait également comparer les vocabulaires d'auteurs ayant offert une large contribution à XYZ
dfm_subset(xyz_dfm, auteur %in% c("Bertrand Bergeron", "David Dorais")) |>
textplot_wordcloud(comparison = TRUE)
head(auteurs_table_ord, n = 20)
# On pourrait également comparer les vocabulaires d'auteurs ayant offert une large contribution à XYZ
dfm_subset(xyz_dfm, auteur %in% c("David Clerson", "Emmanuel Bouchard")) |>
textplot_wordcloud(comparison = TRUE)
head(auteurs_table_ord, n = 30)
# On pourrait également comparer les vocabulaires d'auteurs ayant offert une large contribution à XYZ
dfm_subset(xyz_dfm, auteur %in% c("Edem Awumey", "J.D. Kurtness")) |>
textplot_wordcloud(comparison = TRUE)
# On pourrait également comparer les vocabulaires d'auteurs ayant offert une large contribution à XYZ
set.seed(100)
dfm_subset(xyz_dfm, auteur %in% c("Edem Awumey", "J.D. Kurtness")) |>
textplot_wordcloud(comparison = TRUE)
xyz_dfm |> quanteda.textstats::textstat_keyness()
xyz_dfm |> quanteda.textstats::textstat_keyness() |>
textplot_wordcloud(min_count = 2)
xyz_dfm |> quanteda.textstats::textstat_keyness() |>
textplot_wordcloud(tstat, min_count = 2, comparison = FALSE)
xyz_dfm |> quanteda.textstats::textstat_keyness() |>
textplot_wordcloud(min_count = 2, comparison = FALSE)
if(!"quanteda.textstats" %in% rownames(installed.packages())) {install.packages("quanteda.textstats")}
library(quanteda.textstats)
quanteda.textstats::textstat_collocations(xyz_dfm, min_count = 5, tolower = TRUE)
quanteda.textstats::textstat_collocations(xyz_toks, min_count = 5, tolower = TRUE)
xyz_toks |> tokens_remove(lsa::stopwords_fr) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
xyz_toks |>  |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2015) |>
xyz_toks |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2015) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
xyz_toks |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2021) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
# On peut se servir de ce type de recherche pour ensuite créer des mots composés
col <- xyz_toks |> tokens_remove(lsa::stopwords_fr) |> tokens_subset(annee == 2021) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
col <- xyz_toks |> tokens_remove(lsa::stopwords_fr) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE) |>
tokens_compound(xyz_toks)
col <- xyz_toks |> tokens_remove(lsa::stopwords_fr) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE)
tokens_compound(xyz_toks, pattern = col)
tokens_compound(xyz_toks, pattern = col) |> kwic( pattern = "jeune_fille")
xyz_toks |> tokens_remove(lsa::stopwords_fr) |>
quanteda.textstats::textstat_collocations(min_count = 5, tolower = TRUE) |>
tokens_compound(xyz_toks, pattern = .) |> kwic( pattern = "jeune_fille")
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
View(xyz)
# On veut savoir si et combien il y a des cellules sans contenu ("" ou NA)
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
xyz$test <- NA
xyz$test <- NA
xyz$test2 <- ""
View(xyz)
# On veut savoir si et combien il y a des cellules sans contenu ("" ou NA)
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
xyz[, c("test", "test2")] <- NULL
tokens_compound(xyz_toks, pattern = col) |> kwic( pattern = "jeune_fille")
if(!"genderizeR" %in% rownames(installed.packages())) {install.packages("genderizeR")}{install.packages('genderizeR')}
if(!"genderizeR" %in% rownames(installed.packages())) {install.packages("genderizeR")}
if(!"genderizeR" %in% rownames(installed.packages())) {devtools::install_github("kalimu/genderizeR")}
if(!"devtools" %in% rownames(installed.packages())) {install.packages("devtools")}
if(!"genderizeR" %in% rownames(installed.packages())) {devtools::install_github("kalimu/genderizeR")}
library(genderizeR)           # Extension qui permet d'attribuer un genre à des individus en fonction de leur prénom
#### Ajout d'une colonne `sexe`
xyz$auteur[1:3]
#### Ajout d'une colonne `sexe`
xyz$auteur[1:100]
grep(xyz$auteur[1:5], "\w+", value = TRUE)
grep(xyz$auteur[1:5], "\\w+", value = TRUE)
sapply(xyz$auteur, grep, "\\W+")
sapply(xyz$auteur[1:10], grep, "\\w+")
sapply(xyz$auteur[1:10], grep, "\\[[:alpha:]]+")
sapply(xyz$auteur[1:10], grep, "[[:alpha:]]+")
strcapture(xyz$auteur[1:5], "\\w+", value = TRUE)
strcapture(xyz$auteur[1:5], "\\w+")
strcapture(pattern = "\\w+", xyz$auteur[1:5])
strcapture(pattern = "\\w+", auteur, xyz)
strcapture(pattern = "\\w+", "auteur", xyz)
sub(pattern = "\\w+", xyz$auteur[1:3])
regexpr(pattern = "\\w+", xyz$auteur[1:3])
gregexpr(pattern = "\\w+", xyz$auteur[1:3])
regexpr(pattern = "\\w+", xyz$auteur[1:3])
regexec(pattern = "\\w+", xyz$auteur[1:3])
regmatches(pattern = "\\w+", xyz$auteur[1:3])
regmatches("\\w+", xyz$auteur[1:3])
sapply(regmatches(xyz$auteur[1:10], regexec("\\w", xyz$auteur[1:10])), "[", 1)
sapply(regmatches(xyz$auteur[1:10], regexec("\\w+", xyz$auteur[1:10])), "[", 1)
sapply(regmatches(xyz$auteur[1:10], regexec("\\w+(-\\w+)", xyz$auteur[1:10])), "[", 1)
sapply(regmatches(xyz$auteur[1:10], regexec("\\w+(-\\w+)?", xyz$auteur[1:10])), "[", 1)
#### Séparation des prénoms et noms des auteur.e.s
xyz$auteur[1:100]
sapply(regmatches(xyz$auteur[1:100], regexec("\\w+(-\\w+)?", xyz$auteur[1:10])), "[", 1)
sapply(regmatches(xyz$auteur[1:100], regexec("\\w+(-\\w+)?", xyz$auteur[1:100])), "[", 1)
xyz$prenom <- str_extract(xyz$auteur, "\\w+(-\\w+)?")
genderize(xyz$prenom[1:5])
genderizeR::findGivenNames(xyz$prenom[1:5])
prenoms_prep <- textPrepare(xyz$prenom[1:5])
prenoms_prep
genderize(xyz$prenom[1])
genderdb <- genderDB(xyz$prenom[1:10])
genderdb <- findGivenNames(xyz$prenom[1:10])
genderize(xyz$prenom[1:10], genderDB = genderdb)
genderdb
genderdb <- findGivenNames(xyz$prenom[1:5], language = "fr", country = "CA")
genderdb
genderize(xyz$prenom[1:5], genderDB = genderdb)
prenoms_pre <- textPrepare(xyz$prenom)
prenoms_pre
genderdb <- findGivenNames(xyz$prenom, country = "CA")
prenoms_gender <- genderize(xyz$prenom, genderDB = genderdb)
prenoms_gender
genderdb
genderdb <- findGivenNames(xyz$prenom, textPrepare = TRUE, country = "CA")
prenoms_gender <- genderize(xyz$prenom, genderDB = genderdb)
prenoms_gender
pren_hommes_qc <- read.csv("donnees/gars1980_2021.csv")
pren_hommes_qc <- read.csv("donnees/gars1980_2021.csv", nrows = -5)
pren_hommes_qc <- read.csv("donnees/gars1980_2021.csv")
pren_femmes_qc <- read.csv("donnees/filles1980_2021.csv")
hommes_nonfemmes <- pren_hommes_qc[!prenom %in% pren_femmes_qc$prenom,]
hommes_nonfemmes <- pren_hommes_qc[!`Prénom.Année` %in% pren_femmes_qc$prenom,]
hommes_nonfemmes <- pren_hommes_qc[!pren_hommes_qc$Prénom.Année %in% pren_femmes_qc$prenom,]
femmes_nonhommes <- pren_femmes_qc[!pren_femmes_qc$prenom %in% pren_hommes_qc$Prénom.Année, ]
xyz$genre_auteur <- ifelse(xyz$prenom %in% hommes_nonfemmes, "homme",
ifelse(xyz$prenom %in% femmes_nonhommes, "femme", "so"))
View(xyz)
xyz$genre_auteur <- ifelse(tolower(xyz$prenom) %in% hommes_nonfemmes, "homme",
ifelse(tolower(xyz$prenom) %in% femmes_nonhommes, "femme", "so"))
View(xyz)
hommes_nonfemmes[1:5]
xyz$genre_auteur <- ifelse(tolower(xyz$prenom) %in% tolower(hommes_nonfemmes), "homme",
ifelse(tolower(xyz$prenom) %in% tolower(femmes_nonhommes), "femme", "so"))
View(xyz)
tolower(hommes_nonfemmes)
tolower(xyz$prenom)
tolower(hommes_nonfemmes)
xyz$genre_auteur <- ifelse(toupper(xyz$prenom) %in% hommes_nonfemmes, "homme",
ifelse(toupper(xyz$prenom) %in% femmes_nonhommes, "femme", "so"))
xyz$genre_auteur
library(gender)
install.packages("gender")
library(gender)
gender(xyz$prenom[1:5])
remotes::install_github("lmullen/genderdata")
pren_hommes_qc$Prénom.Année
pren_hommes_qc$Prénom.Année[2000:2050]
pren_hommes_qc$Prénom.Année[20000:20050]
pren_hommes_qc$Prénom.Année[50000:50050]
pren_hommes_qc
pren_femmes_qc
"camille" %ilike% pren_femmes_qc
"camille" %in% pren_femmes_qc
"CAMILLE" %in% pren_femmes_qc
prenoms_genres <- read.csv("donnees/prenoms_genres_qc_fr.csv")
prenoms_genres
unique(prenoms_genres)
prenoms_genres_uniques <- unique(prenoms_genres)
xyz$genre_auteur <- ifelse(xyz$prenom %in% prenoms_genres_uniques$prenom, prenoms_genres_uniques$genre_auteur, "inconnu")
xyz$genre_auteur
View(xyz)
dictionnaire = dictionary(list(ville = c("Montréal", "métropole", "ville", "cité", "urbain","capitale"),
province = c("Québec","province"),
pays = c("Canada", "pays")))
View(xyz)
corp_2015 <- corpus_subset(data_corpus_inaugural, Year == 2015)
corp_2021 <- corpus_subset(data_corpus_inaugural, Year == 2021)
head(dfm(tokens(corp_2015), dictionary = dictionnaire))
corp_2015 <- corpus_subset(xyz_corp, Year == 2015)
corp_2021 <- corpus_subset(xyz_corp, Year == 2021)
corp_2015 <- corpus(xyz_corp) |> corpus_subset(Year == 2015)
corp_2015 <- corpus(xyz_corp) |> corpus_subset(annee == 2015)
corp_2021 <- corpus(xyz_corp) |> corpus_subset(annee == 2021)
head(dfm_lookup(dfm(corp_2015)), dictionary = dictionnaire)
head(dfm_lookup(dfm(tokens(corp_2015))), dictionary = dictionnaire)
dictionnaire = dictionary(list(ville = c("Montréal", "métropole"),
province = c("Québec","province"),
pays = c("Canada", "pays")))
head(dfm_lookup(dfm(tokens(corp_2015))), dictionary = dictionnaire)
head(dfm_lookup(dfm(tokens(corp_2015)), dictionary = dictionnaire))
head(dfm_lookup(dfm(tokens(corp_2021)), dictionary = dictionnaire))
dfm_lookup(dfm(tokens(corp_2015)), dictionary = dictionnaire)
setwd("~/github/LADIREC/ladirec_classification_final/corpus_fr")
# Vérification du répertoire de travail
getwd()
if(!"stringr" %in% rownames(installed.packages())) {install.packages("stringr")}
if(!"readxl" %in% rownames(installed.packages())) {install.packages("readxl")}
if(!"dplyr" %in% rownames(installed.packages())) {install.packages("dplyr")}
if(!"ggplot2" %in% rownames(installed.packages())) {install.packages("ggplot2")}
if(!"quanteda" %in% rownames(installed.packages())) {install.packages("quanteda")}
if(!"quanteda.textplots" %in% rownames(installed.packages())) {install.packages("quanteda.textplots")}
if(!"quanteda.textstats" %in% rownames(installed.packages())) {install.packages("quanteda.textstats")}
if(!"lsa" %in% rownames(installed.packages())) {install.packages("lsa")}
# Lecture des données et assignation à une variable
xyz <- read_excel("donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)
library(readxl)               # Extension pour l'importation de fichiers Excel
library(stringr)              # Extension pour la manipulation des chaînes de caractères
library(dplyr)                # Extension pour la manipulation des structures des tableaux de données
library(ggplot2)              # Extension pour la production de graphiques de haute qualité
library(quanteda)             # Extension pour le forage textuel
library(quanteda.textplots)   # Extension pour le forage textuel
library(quanteda.textstats)   # Extension pour l'analyse des collocations
library(lsa)
# Lecture des données et assignation à une variable
xyz <- read_excel("donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)
# Lecture des données et assignation à une variable
xyz <- readxl::read_excel("donnees/XYZ-2015-2022-table-20230205JV.xlsx", sheet = 1)
# Examen de la structure
str(xyz)
# On peut observer les détails de cette structure une information à la fois
# Nom des colonnes
colnames(xyz)
# Nombre de lignes et de colonnes dans le tableau
dim(xyz)
# Nombre de lignes
nrow(xyz)
# Regardons de quels types sont les colonnes
class(xyz$`ISSN (numérique)`)
# On veut savoir si et combien il y a des cellules sans contenu ("" ou NA)
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# Facultatif: on peut créer deux colonnes sans contenu vérifier à nouveau si R les repère
xyz$test <- NA
xyz$test2 <- ""
xyz$test
xyz
sapply(xyz, function(x) sum(is.na(x)))
sapply(xyz, function(x) sum(x == ""))
# On supprime ces colonnes inutiles
xyz[, c("test", "test2")] <- NULL
# Vérification
xyz
xyz
colnames(xyz)
xyz
colnames(xyz)
# 1. Renommer les colonnes
colnames(xyz) <- c("periodique", "titre", "auteur",
"numero", "date", "theme", "uri",
"editeur", "issn_imp", "issn_num",
"citation", "mention_legale", "texte")
xyz
xyz
nrow(xyz)
1:nrow(xyz)
xyz$doc_id <- 1:nrow(xyz)
xyz
unique(xyz[ , "periodique"])
xyz[ , "periodique"]
unique(xyz[ , "periodique"])
table(xyz$periodique)
# Créons un vecteur avec les colonnes inutiles (on utilise pour cela la fonction de concaténation c( ) )
colonnes_a_supprimer <- c("periodique", "editeur", "issn_imp", "issn_num", "mention_legale", "uri", "citation")
# On élimine l'ensemble des colonnes inutiles d'un seul coup.
xyz[, colonnes_a_supprimer] <- NULL
xyz
# 6. Remplacer un symbole dans une longue chaine de caractères (un texte)
# Observons tout d'abord un texte en particulier
xyz$texte[1]
# Remplaçons par une espace simple le symbole `\n`
xyz$texte <- gsub(pattern = "\n", replacement = " ", x = xyz$texte, fixed = TRUE)
xyz
# Vérifions à nouveau un texte pris au hasard
xyz$texte[3]
table(xyz$annee)
xyz
# 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- stringr::str_extract(xyz$date, "[0-9]+")
xyz
xyz$annee <- as.integer(xyz$annee)
xyz
# 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- regexpr(pattern = "[0-9]+", x=xyz$date)
# 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- regexpr(pattern = "[0-9]+", text = xyz$date)
xyz$annee
# 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- regexpr(pattern = "[0-9]+", text = xyz$date, perl = TRUE)
xyz$annee
# 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- stringr::str_extract(xyz$date, "[0-9]+")
xyz$annee
# 4. On veut créer une colonne contenant les années. Il faut les extraire de la colonne `date`.
xyz$annee <- regmatches(xyz$date, "[0-9]+")
xyz$annee <- as.integer(xyz$annee)
# 6. Remplacer un symbole dans une longue chaine de caractères (un texte)
# Observons tout d'abord un texte en particulier
xyz$texte[1]
#### Exploration 1: les métadonnées ----
# La fonction `table( )` permet d'observer la fréquence des modalités d'un ou de plusieurs champs
distrib_annuelle <- table(xyz$annee)
distrib_annuelle
# Quelle est la moyenne de cette distribution?
mean(distrib_annuelle)
table(xyz$auteur)
# 1. Syntaxe étendue (petite perte de mémoire, mais plus explicite)
distrib_auteurs <- table(xyz$auteur)
distrib_auteurs_ord <- sort(distrib_auteurs, decreasing = TRUE)
head(distrib_auteurs_ord, n = 10)
# 2. Syntaxe condensée, par enchâssement des fonctions. Lire de l'intérieur vers l'extérieur
head(sort(table(xyz$auteur), decreasing = TRUE), 10)
# Une tâche importante dans l'AED est de comprendre s'il existe des corrélations entre des variables
# Les corrélations se calculent sur des variables numériques. Soit par exemple le jeu de données `mtcars`
mtcars
# On voudrait savoir s'il y a une corrélation, positive ou négative, entre le volume du moteur (disp) et la distance/gallon que peut parcourir une voiture
plot(mtcars$disp, mtcars$mpg)
# Cette corrélation est confirmée par la p-value
cor.test(mtcars$disp, mtcars$mpg)
N_mots_f <- function(x){
mots_v <- strsplit(x, "[' ]") |> unlist()
non_vide_v <- which(mots_v != "")
mots_v <- mots_v[non_vide_v]
return(length(mots_v))
}
xyz$titre_Nmots <- lapply(
xyz$titre, N_mots_f
) |> unlist()
xyz
xyz$texte_Nmots <- lapply(
xyz$texte, N_mots_f
) |> unlist()
xyz
# Application de cette fonction aux titres et aux textes
xyz$titre_Nmots <- sapply(
xyz$titre, N_mots_f
)
xyz
xyz$texte_Nmots <- sapply(
xyz$texte, N_mots_f
)
xyz
sapply(
xyz$texte, N_mots_f
)
N_mots_f <- function(x){
mots_v <- strsplit(x, "[' ]") |> unlist()
non_vide_v <- which(mots_v != "")
mots_v <- mots_v[non_vide_v]
nbre_mots <- length(mots_v)
return(nbre_mots)
}
sapply(
xyz$titre, N_mots_f
)
xyz$texte_Nmots
N_mots_f <- function(x){
mots_v <- strsplit(x, "[' ]") |> unlist()
non_vide_v <- which(mots_v != "")
mots_v <- mots_v[non_vide_v]
nbre_mots <- length(mots_v)
names(nbre_mots) <- NULL
return(nbre_mots)
}
xyz$texte_Nmots
sapply(
xyz$titre, N_mots_f
)
